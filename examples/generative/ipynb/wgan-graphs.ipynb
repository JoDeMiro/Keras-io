{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# WGAN-GP with R-GCN for the generation of small molecular graphs\n",
    "\n",
    "**Author:** [akensert](https://github.com/akensert)<br>\n",
    "**Date created:** 2021/06/30<br>\n",
    "**Last modified:** 2021/06/30<br>\n",
    "**Description:** Complete implementation of WGAN-GP with R-GCN to generate novel molecules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we implement a generative model for graphs and use it to generate\n",
    "novel molecules.\n",
    "\n",
    "Motivation: The [development of new drugs](https://en.wikipedia.org/wiki/Drug_development)\n",
    "(molecules) can be extremely time-consuming and costly. The use of deep learning models\n",
    "can alleviate the search for good candidate drugs, by predicting properties of known molecules\n",
    "(e.g., solubility, toxicity, affinity to target protein, etc.). As the number of\n",
    "possible molecules is astronomical, the space in which we search for/explore molecules is\n",
    "just a fraction of the entire space. Therefore, it's arguably desirable to implement\n",
    "generative models that can learn to generate novel molecules (which would otherwise have never been explored).\n",
    "\n",
    "### References (implementation)\n",
    "\n",
    "The implementation in this tutorial is based on/inspired by the\n",
    "[MolGAN paper](https://arxiv.org/abs/1805.11973) and DeepChem's\n",
    "[Basic MolGAN](https://deepchem.readthedocs.io/en/latest/api_reference/models.html#basicmolganmod\n",
    "el).\n",
    "\n",
    "### Further reading (generative models)\n",
    "Recent implementations of generative models for molecular graphs also include\n",
    "[Mol-CycleGAN](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-019-0404-1),\n",
    "[GraphVAE](https://arxiv.org/abs/1802.03480) and\n",
    "[JT-VAE](https://arxiv.org/abs/1802.04364). For more information on generative\n",
    "adverserial networks, see [GAN](https://arxiv.org/abs/1406.2661),\n",
    "[WGAN](https://arxiv.org/abs/1701.07875) and [WGAN-GP](https://arxiv.org/abs/1704.00028)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Install RDKit\n",
    "\n",
    "[RDKit](https://www.rdkit.org/) is a collection of cheminformatics and machine-learning\n",
    "software written in C++ and Python. In this tutorial, RDKit is used to conviently and\n",
    "efficiently transform\n",
    "[SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) to\n",
    "molecule objects, and then from those obtain sets of atoms and bonds.\n",
    "\n",
    "SMILES expresses the structure of a given molecule in the form of an ASCII string.\n",
    "The SMILES string is a compact encoding which, for smaller molecules, is relatively\n",
    "human-readable. Encoding molecules as a string both alleviates and facilitates database\n",
    "and/or web searching of a given molecule. RDKit uses algorithms to\n",
    "accurately transform a given SMILES to a molecule object, which can then\n",
    "be used to compute a great number of molecular properties/features.\n",
    "\n",
    "Notice, RDKit is commonly installed via [Conda](https://www.rdkit.org/docs/Install.html).\n",
    "However, thanks to\n",
    "[rdkit_platform_wheels](https://github.com/kuelumbus/rdkit_platform_wheels), rdkit\n",
    "can now (for the sake of this tutorial) be installed easily via pip, as follows:\n",
    "```\n",
    "pip -q install rdkit-pypi\n",
    "```\n",
    "And to allow easy visualization of a molecule objects, Pillow needs to be installed:\n",
    "```\n",
    "pip -q install Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem.Draw import IPythonConsole, MolsToGridImage\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used in this tutorial is a\n",
    "[quantum mechanics dataset](http://quantum-machine.org/datasets/) (QM9), obtained from\n",
    "[MoleculeNet](http://moleculenet.ai/datasets-1). Although many feature and label columns\n",
    "come with the dataset, we'll only focus on the\n",
    "[SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system)\n",
    "column. The QM9 dataset is a good first dataset to work with for generating\n",
    "graphs, as the maximum number of heavy (non-hydrogen) atoms found in a molecule is only nine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "csv_path = tf.keras.utils.get_file(\n",
    "    \"qm9.csv\", \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\"\n",
    ")\n",
    "\n",
    "data = []\n",
    "with open(csv_path, \"r\") as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        data.append(line.split(\",\")[1])\n",
    "\n",
    "# Let's look at a molecule of the dataset\n",
    "smiles = data[1000]\n",
    "print(\"SMILES:\", smiles)\n",
    "molecule = Chem.MolFromSmiles(smiles)\n",
    "print(\"Num heavy atoms:\", molecule.GetNumHeavyAtoms())\n",
    "molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Define helper functions\n",
    "These helper functions will help convert SMILES to graphs and graphs to molecule objects.\n",
    "\n",
    "**Representing a molecular graph**. Molecules can naturally be expressed as undirected\n",
    "graphs `G = (V, E)`, where `V` is a set of vertices (atoms), and `E` a set of edges\n",
    "(bonds). As for this implementation, each graph (molecule) will be represented as an\n",
    "adjacency tensor `A`, which encodes existence/non-existence of atom-pairs with their\n",
    "one-hot encoded bond types stretching an extra dimension, and a feature tensor `H`, which\n",
    "for each atom, one-hot encodes its atom type. Notice, as hydrogen atoms can be inferred by\n",
    "RDKit, hydrogen atoms are excluded from `A` and `H` for easier modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "atom_mapping = {\n",
    "    \"C\": 0,\n",
    "    0: \"C\",\n",
    "    \"N\": 1,\n",
    "    1: \"N\",\n",
    "    \"O\": 2,\n",
    "    2: \"O\",\n",
    "    \"F\": 3,\n",
    "    3: \"F\",\n",
    "}\n",
    "\n",
    "bond_mapping = {\n",
    "    \"SINGLE\": 0,\n",
    "    0: Chem.BondType.SINGLE,\n",
    "    \"DOUBLE\": 1,\n",
    "    1: Chem.BondType.DOUBLE,\n",
    "    \"TRIPLE\": 2,\n",
    "    2: Chem.BondType.TRIPLE,\n",
    "    \"AROMATIC\": 3,\n",
    "    3: Chem.BondType.AROMATIC,\n",
    "}\n",
    "\n",
    "NUM_ATOMS = 9  # Maximum number of atoms\n",
    "ATOM_DIM = 4 + 1  # Number of atom types\n",
    "BOND_DIM = 4 + 1  # Number of bond types\n",
    "LATENT_DIM = 64  # Size of the latent space\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    # Converts SMILES to molecule object\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Initialize adjacency and feature tensor\n",
    "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
    "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
    "\n",
    "    # loop over each atom in molecule\n",
    "    for atom in molecule.GetAtoms():\n",
    "        i = atom.GetIdx()\n",
    "        atom_type = atom_mapping[atom.GetSymbol()]\n",
    "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
    "        # loop over one-hop neighbors\n",
    "        for neighbor in atom.GetNeighbors():\n",
    "            j = neighbor.GetIdx()\n",
    "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
    "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
    "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
    "\n",
    "    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n",
    "    # Notice: channels-first\n",
    "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
    "\n",
    "    # Where no atom, add 1 to last column (indicating \"non-atom\")\n",
    "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
    "\n",
    "    return adjacency, features\n",
    "\n",
    "\n",
    "def graph_to_molecule(graph):\n",
    "    # Unpack graph\n",
    "    adjacency, features = graph\n",
    "\n",
    "    # RWMol is a molecule object intended to be edited\n",
    "    molecule = Chem.RWMol()\n",
    "\n",
    "    # Remove \"no atoms\" & atoms with no bonds\n",
    "    keep_idx = np.where(\n",
    "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
    "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
    "    )[0]\n",
    "    features = features[keep_idx]\n",
    "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
    "\n",
    "    # Add atoms to molecule\n",
    "    for atom_type_idx in np.argmax(features, axis=1):\n",
    "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
    "        _ = molecule.AddAtom(atom)\n",
    "\n",
    "    # Add bonds between atoms in molecule; based on the upper triangles\n",
    "    # of the [symmetric] adjacency tensor\n",
    "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
    "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
    "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
    "            continue\n",
    "        bond_type = bond_mapping[bond_ij]\n",
    "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
    "\n",
    "    # Sanitize the molecule; for more information on sanitization, see\n",
    "    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
    "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "    # Let's be strict. If sanitization fails, return None\n",
    "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "        return None\n",
    "\n",
    "    return molecule\n",
    "\n",
    "\n",
    "# Test helper functions\n",
    "graph_to_molecule(smiles_to_graph(smiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Generate training set\n",
    "\n",
    "To save training time, we'll only use a tenth of the QM9 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "adjacency_tensor, feature_tensor = [], []\n",
    "for smiles in data[::10]:\n",
    "    adjacency, features = smiles_to_graph(smiles)\n",
    "    adjacency_tensor.append(adjacency)\n",
    "    feature_tensor.append(features)\n",
    "\n",
    "adjacency_tensor = np.array(adjacency_tensor)\n",
    "feature_tensor = np.array(feature_tensor)\n",
    "\n",
    "print(\"adjacency_tensor.shape =\", adjacency_tensor.shape)\n",
    "print(\"feature_tensor.shape =\", feature_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model\n",
    "\n",
    "The idea is to implement a generator network and a discriminator network via WGAN-GP,\n",
    "that will result in a generator network that can generate small novel molecules\n",
    "(small graphs).\n",
    "\n",
    "The generator network needs to be able to map (for each example in the batch) a vector `z`\n",
    "to a 3-D adjacency tensor (`A`) and 2-D feature tensor (`H`). For this, `z` will first be\n",
    "passed through a fully-connected network, for which the output will be further passed\n",
    "through two separate fully-connected networks. Each of these two fully-connected\n",
    "networks will then output (for each example in the batch) a tanh-activated vector\n",
    "followed by a reshape and softmax to match that of a multi-dimensional adjacency/feature\n",
    "tensor.\n",
    "\n",
    "As the discriminator network will recieves as input a graph (`A`, `H`) from either the\n",
    "genrator or from the training set, we'll need to implement graph convolutional layers,\n",
    "which allows us to operate on graphs. This means that input to the discriminator network\n",
    "will first pass through graph convolutional layers, then an average-pooling layer,\n",
    "and finally a few fully-connected layers. The final output should be a scalar (for each\n",
    "example in the batch) which indicates the \"realness\" of the associated input\n",
    "(in this case a \"fake\" or \"real\" molecule).\n",
    "\n",
    "\n",
    "### Graph generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def GraphGenerator(\n",
    "    dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape,\n",
    "):\n",
    "    z = keras.layers.Input(shape=(LATENT_DIM,))\n",
    "    # Propagate through one or more densely connected layers\n",
    "    x = z\n",
    "    for units in dense_units:\n",
    "        x = keras.layers.Dense(units, activation=\"tanh\")(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n",
    "    x_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)\n",
    "    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)\n",
    "    # Symmetrify tensors in the last two dimensions\n",
    "    x_adjacency = (x_adjacency + tf.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n",
    "    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
    "    x_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)\n",
    "    x_features = keras.layers.Reshape(feature_shape)(x_features)\n",
    "    x_features = keras.layers.Softmax(axis=2)(x_features)\n",
    "\n",
    "    return keras.Model(inputs=z, outputs=[x_adjacency, x_features], name=\"Generator\")\n",
    "\n",
    "\n",
    "generator = GraphGenerator(\n",
    "    dense_units=[128, 256, 512],\n",
    "    dropout_rate=0.2,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Graph discriminator\n",
    "\n",
    "\n",
    "**Graph convolutional layer**. The\n",
    "[relational graph convolutional layers](https://arxiv.org/abs/1703.06103) implements non-linearly transformed\n",
    "neighborhood aggregations. We can define these layers as follows:\n",
    "\n",
    "`H^{l+1} = σ(D^{-1} @ A @ H^{l+1} @ W^{l})`\n",
    "\n",
    "\n",
    "Where `σ` denotes the non-linear transformation (commonly a ReLU activation), `A` the\n",
    "adjacency tensor, `H^{l}` the feature tensor at the `l:th` layer, `D^{-1}` the inverse\n",
    "diagonal degree tensor of `A`, and `W^{l}` the trainable weight tensor at the `l:th`\n",
    "layer. Specifically, for each bond type (relation), the degree tensor expresses, in the\n",
    "diagonal, the number of bonds attached to each atom. Notice, in this tutorial `D^{-1}` is\n",
    "omitted, for two reasons: (1) it's not obvious how to apply this normalization on the\n",
    "continuous adjacency tensors (generated by the generator), and (2) the performance of the\n",
    "WGAN without normalization seems to work just fine. Furthermore, in contrast to the\n",
    "[original paper](https://arxiv.org/abs/1703.06103), no self-loop is defined, as we don't\n",
    "want to train the generator to predict \"self-bonding\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RelationalGraphConvLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units=128,\n",
    "        activation=\"relu\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        bond_dim = input_shape[0][1]\n",
    "        atom_dim = input_shape[1][2]\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(bond_dim, atom_dim, self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            trainable=True,\n",
    "            name=\"W\",\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(bond_dim, 1, self.units),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                trainable=True,\n",
    "                name=\"b\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        adjacency, features = inputs\n",
    "        # Aggregate information from neighbors\n",
    "        x = tf.matmul(adjacency, features[:, None, :, :])\n",
    "        # Apply linear transformation\n",
    "        x = tf.matmul(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            x += self.bias\n",
    "        # Reduce bond types dim\n",
    "        x_reduced = tf.reduce_sum(x, axis=1)\n",
    "        # Apply non-linear transformation\n",
    "        return self.activation(x_reduced)\n",
    "\n",
    "\n",
    "def GraphDiscriminator(\n",
    "    gconv_units, dense_units, dropout_rate, adjacency_shape, feature_shape\n",
    "):\n",
    "\n",
    "    adjacency = keras.layers.Input(shape=adjacency_shape)\n",
    "    features = keras.layers.Input(shape=feature_shape)\n",
    "\n",
    "    # Propagate through one or more graph convolutional layers\n",
    "    features_transformed = features\n",
    "    for units in gconv_units:\n",
    "        features_transformed = RelationalGraphConvLayer(units)(\n",
    "            [adjacency, features_transformed]\n",
    "        )\n",
    "\n",
    "    # Reduce 2-D representation of molecule to 1-D\n",
    "    x = keras.layers.GlobalAveragePooling1D()(features_transformed)\n",
    "\n",
    "    # Propagate through one or more densely connected layers\n",
    "    for units in dense_units:\n",
    "        x = keras.layers.Dense(units, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # For each molecule, output a single scalar value expressing the\n",
    "    # \"realness\" of the inputted molecule\n",
    "    x_out = keras.layers.Dense(1, dtype=\"float32\")(x)\n",
    "\n",
    "    return keras.Model(inputs=[adjacency, features], outputs=x_out)\n",
    "\n",
    "\n",
    "discriminator = GraphDiscriminator(\n",
    "    gconv_units=[128, 128, 128, 128],\n",
    "    dense_units=[512, 512],\n",
    "    dropout_rate=0.2,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GraphWGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        discriminator_steps=1,\n",
    "        generator_steps=1,\n",
    "        gp_weight=10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator_steps = discriminator_steps\n",
    "        self.generator_steps = generator_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.latent_dim = self.generator.input_shape[-1]\n",
    "\n",
    "    def compile(self, optimizer_generator, optimizer_discriminator, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer_generator = optimizer_generator\n",
    "        self.optimizer_discriminator = optimizer_discriminator\n",
    "        self.metric_generator = keras.metrics.Mean(name=\"loss_gen\")\n",
    "        self.metric_discriminator = keras.metrics.Mean(name=\"loss_dis\")\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "\n",
    "        if isinstance(inputs[0], tuple):\n",
    "            inputs = inputs[0]\n",
    "\n",
    "        graph_real = inputs\n",
    "\n",
    "        self.batch_size = tf.shape(inputs[0])[0]\n",
    "\n",
    "        # Train the discriminator for one or more steps\n",
    "        for _ in range(self.discriminator_steps):\n",
    "            z = tf.random.normal((self.batch_size, self.latent_dim))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                graph_generated = self.generator(z, training=True)\n",
    "                loss = self._loss_discriminator(graph_real, graph_generated)\n",
    "\n",
    "            grads = tape.gradient(loss, self.discriminator.trainable_weights)\n",
    "            self.optimizer_discriminator.apply_gradients(\n",
    "                zip(grads, self.discriminator.trainable_weights)\n",
    "            )\n",
    "            self.metric_discriminator.update_state(loss)\n",
    "\n",
    "        # Train the generator for one or more steps\n",
    "        for _ in range(self.generator_steps):\n",
    "            z = tf.random.normal((self.batch_size, self.latent_dim))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                graph_generated = self.generator(z, training=True)\n",
    "                loss = self._loss_generator(graph_generated)\n",
    "\n",
    "                grads = tape.gradient(loss, self.generator.trainable_weights)\n",
    "                self.optimizer_generator.apply_gradients(\n",
    "                    zip(grads, self.generator.trainable_weights)\n",
    "                )\n",
    "                self.metric_generator.update_state(loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def _loss_discriminator(self, graph_real, graph_generated):\n",
    "        logits_real = self.discriminator(graph_real, training=True)\n",
    "        logits_generated = self.discriminator(graph_generated, training=True)\n",
    "        loss = tf.reduce_mean(logits_generated) - tf.reduce_mean(logits_real)\n",
    "        loss_gp = self._gradient_penalty(graph_real, graph_generated)\n",
    "        return loss + loss_gp * self.gp_weight\n",
    "\n",
    "    def _loss_generator(self, graph_generated):\n",
    "        logits_generated = self.discriminator(graph_generated, training=True)\n",
    "        return -tf.reduce_mean(logits_generated)\n",
    "\n",
    "    def _gradient_penalty(self, graph_real, graph_generated):\n",
    "        # Unpack graphs\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_generated, features_generated = graph_generated\n",
    "\n",
    "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
    "        alpha = tf.random.uniform([self.batch_size])\n",
    "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
    "        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n",
    "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))\n",
    "        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n",
    "\n",
    "        # Compute the logits of interpolated graphs\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adjacency_interp)\n",
    "            tape.watch(features_interp)\n",
    "            logits = self.discriminator(\n",
    "                [adjacency_interp, features_interp], training=True\n",
    "            )\n",
    "\n",
    "        # Compute the gradients with respect to the interpolated graphs\n",
    "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
    "        # Compute the gradient penalty\n",
    "        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2\n",
    "        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2\n",
    "        return tf.reduce_mean(\n",
    "            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))\n",
    "            + tf.reduce_mean(grads_features_penalty, axis=(-1))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "To save time (if run on a CPU), we'll only train the model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "wgan = GraphWGAN(generator, discriminator, discriminator_steps=1)\n",
    "\n",
    "wgan.compile(\n",
    "    optimizer_generator=keras.optimizers.Adam(5e-4),\n",
    "    optimizer_discriminator=keras.optimizers.Adam(5e-4),\n",
    ")\n",
    "\n",
    "wgan.fit([adjacency_tensor, feature_tensor], epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Sample novel molecules with the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample(generator, batch_size):\n",
    "    z = tf.random.normal((batch_size, LATENT_DIM))\n",
    "    graph = generator.predict(z)\n",
    "    # obtain one-hot encoded adjacency tensor\n",
    "    adjacency = tf.argmax(graph[0], axis=1)\n",
    "    adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
    "    # Remove potential self-loops from adjacency\n",
    "    adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
    "    # obtain one-hot encoded feature tensor\n",
    "    features = tf.argmax(graph[1], axis=2)\n",
    "    features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
    "    return [\n",
    "        graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
    "        for i in range(batch_size)\n",
    "    ]\n",
    "\n",
    "\n",
    "molecules = sample(wgan.generator, batch_size=48)\n",
    "\n",
    "MolsToGridImage(\n",
    "    [m for m in molecules if m is not None][:25], molsPerRow=5, subImgSize=(150, 150)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Concluding thoughts\n",
    "\n",
    "**Inspecting the results**. Ten epochs of training seemed enough to generate some decent\n",
    "looking molecules! Notice, in contrast to the\n",
    "[MolGAN paper](https://arxiv.org/abs/1805.11973), the uniqueness of the generated\n",
    "molecules in this tutorial seems really high, which is great!\n",
    "\n",
    "**What we've learned, and prospects**. In this tutorial, a generative model for molecular\n",
    "graphs was succesfully implemented, which allowed us to generate novel molecules. In the\n",
    "future, it would be interesting to implement generative models that can modify existing\n",
    "molecules (for instance, to optimize solubility or protein-binding of an existing\n",
    "molecule). For that however, a reconstruction loss would likely be needed, which is\n",
    "tricky to implement as there's no easy and obvious way to compute similarity between two\n",
    "molecular graphs.\n",
    "\n",
    "Example available on HuggingFace\n",
    "\n",
    "| Trained Model | Demo |\n",
    "| :--: | :--: |\n",
    "| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-wgan%20graphs-black.svg)](https://huggingface.co/keras-io/wgan-molecular-graphs) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-wgan%20graphs-black.svg)](https://huggingface.co/spaces/keras-io/Generating-molecular-graphs-by-WGAN-GP) |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wgan-graphs",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}