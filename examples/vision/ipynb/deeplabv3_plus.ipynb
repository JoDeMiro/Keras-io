{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Multiclass semantic segmentation using DeepLabV3+\n",
    "\n",
    "**Author:** [Soumik Rakshit](http://github.com/soumik12345)<br>\n",
    "**Date created:** 2021/08/31<br>\n",
    "**Last modified:** 2021/09/1<br>\n",
    "**Description:** Implement DeepLabV3+ architecture for Multi-class Semantic Segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Semantic segmentation, with the goal to assign semantic labels to every pixel in an image,\n",
    "is an essential computer vision task. In this example, we implement\n",
    "the **DeepLabV3+** model for multi-class semantic segmentation, a fully-convolutional\n",
    "architecture that performs well on semantic segmentation benchmarks.\n",
    "\n",
    "### References:\n",
    "\n",
    "- [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1802.02611.pdf)\n",
    "- [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)\n",
    "- [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Downloading the data\n",
    "\n",
    "We will use the [Crowd Instance-level Human Parsing Dataset](https://arxiv.org/abs/1811.12596)\n",
    "for training our model. The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images.\n",
    "Each image in CIHP is labeled with pixel-wise annotations for 20 categories, as well as instance-level identification.\n",
    "This dataset can be used for the \"human part segmentation\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz\n",
    "!unzip -q instance-level-human-parsing.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Creating a TensorFlow Dataset\n",
    "\n",
    "Training on the entire CIHP dataset with 38,280 images takes a lot of time, hence we will be using\n",
    "a smaller subset of 200 images for training our model in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 20\n",
    "DATA_DIR = \"./instance-level_human_parsing/instance-level_human_parsing/Training\"\n",
    "NUM_TRAIN_IMAGES = 1000\n",
    "NUM_VAL_IMAGES = 50\n",
    "\n",
    "train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[:NUM_TRAIN_IMAGES]\n",
    "train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[:NUM_TRAIN_IMAGES]\n",
    "val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n",
    "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
    "]\n",
    "val_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n",
    "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
    "]\n",
    "\n",
    "\n",
    "def read_image(image_path, mask=False):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    if mask:\n",
    "        image = tf.image.decode_png(image, channels=1)\n",
    "        image.set_shape([None, None, 1])\n",
    "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    else:\n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "        image.set_shape([None, None, 3])\n",
    "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "        image = image / 127.5 - 1\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_data(image_list, mask_list):\n",
    "    image = read_image(image_list)\n",
    "    mask = read_image(mask_list, mask=True)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def data_generator(image_list, mask_list):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = data_generator(train_images, train_masks)\n",
    "val_dataset = data_generator(val_images, val_masks)\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Val Dataset:\", val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Building the DeepLabV3+ model\n",
    "\n",
    "DeepLabv3+ extends DeepLabv3 by adding an encoder-decoder structure. The encoder module\n",
    "processes multiscale contextual information by applying dilated convolution at multiple\n",
    "scales, while the decoder module refines the segmentation results along object boundaries.\n",
    "\n",
    "![](https://github.com/lattice-ai/DeepLabV3-Plus/raw/master/assets/deeplabv3_plus_diagram.png)\n",
    "\n",
    "**Dilated convolution:** With dilated convolution, as we go deeper in the network, we can keep the\n",
    "stride constant but with larger field-of-view without increasing the number of parameters\n",
    "or the amount of computation. Besides, it enables larger output feature maps, which is\n",
    "useful for semantic segmentation.\n",
    "\n",
    "The reason for using **Dilated Spatial Pyramid Pooling** is that it was shown that as the\n",
    "sampling rate becomes larger, the number of valid filter weights (i.e., weights that\n",
    "are applied to the valid feature region, instead of padded zeros) becomes smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def convolution_block(\n",
    "    block_input,\n",
    "    num_filters=256,\n",
    "    kernel_size=3,\n",
    "    dilation_rate=1,\n",
    "    padding=\"same\",\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding=\"same\",\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=keras.initializers.HeNormal(),\n",
    "    )(block_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input):\n",
    "    dims = dspp_input.shape\n",
    "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = layers.UpSampling2D(\n",
    "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The encoder features are first bilinearly upsampled by a factor 4, and then\n",
    "concatenated with the corresponding low-level features from the network backbone that\n",
    "have the same spatial resolution. For this example, we\n",
    "use a ResNet50 pretrained on ImageNet as the backbone model, and we use\n",
    "the low-level features from the `conv4_block6_2_relu` block of the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def DeeplabV3Plus(image_size, num_classes):\n",
    "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    resnet50 = keras.applications.ResNet50(\n",
    "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x)\n",
    "\n",
    "    input_a = layers.UpSampling2D(\n",
    "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "    x = convolution_block(x)\n",
    "    x = convolution_block(x)\n",
    "    x = layers.UpSampling2D(\n",
    "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
    "    return keras.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "\n",
    "model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training\n",
    "\n",
    "We train the model using sparse categorical crossentropy as the loss function, and\n",
    "Adam as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=25)\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.ylabel(\"val_loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.ylabel(\"val_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Inference using Colormap Overlay\n",
    "\n",
    "The raw predictions from the model represent a one-hot encoded tensor of shape `(N, 512, 512, 20)`\n",
    "where each one of the 20 channels is a binary mask corresponding to a predicted label.\n",
    "In order to visualize the results, we plot them as RGB segmentation masks where each pixel\n",
    "is represented by a unique color corresponding to the particular label predicted. We can easily\n",
    "find the color corresponding to each label from the `human_colormap.mat` file provided as part\n",
    "of the dataset. We would also plot an overlay of the RGB segmentation mask on the input image as\n",
    "this further helps us to identify the different categories present in the image more intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Loading the Colormap\n",
    "colormap = loadmat(\n",
    "    \"./instance-level_human_parsing/instance-level_human_parsing/human_colormap.mat\"\n",
    ")[\"colormap\"]\n",
    "colormap = colormap * 100\n",
    "colormap = colormap.astype(np.uint8)\n",
    "\n",
    "\n",
    "def infer(model, image_tensor):\n",
    "    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n",
    "    predictions = np.squeeze(predictions)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def decode_segmentation_masks(mask, colormap, n_classes):\n",
    "    r = np.zeros_like(mask).astype(np.uint8)\n",
    "    g = np.zeros_like(mask).astype(np.uint8)\n",
    "    b = np.zeros_like(mask).astype(np.uint8)\n",
    "    for l in range(0, n_classes):\n",
    "        idx = mask == l\n",
    "        r[idx] = colormap[l, 0]\n",
    "        g[idx] = colormap[l, 1]\n",
    "        b[idx] = colormap[l, 2]\n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def get_overlay(image, colored_mask):\n",
    "    image = tf.keras.preprocessing.image.array_to_img(image)\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n",
    "    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
    "    for i in range(len(display_list)):\n",
    "        if display_list[i].shape[-1] == 3:\n",
    "            axes[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        else:\n",
    "            axes[i].imshow(display_list[i])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions(images_list, colormap, model):\n",
    "    for image_file in images_list:\n",
    "        image_tensor = read_image(image_file)\n",
    "        prediction_mask = infer(image_tensor=image_tensor, model=model)\n",
    "        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n",
    "        overlay = get_overlay(image_tensor, prediction_colormap)\n",
    "        plot_samples_matplotlib(\n",
    "            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n",
    "        )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Inference on Train Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plot_predictions(train_images[:4], colormap, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Inference on Validation Images\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/deeplabv3p-resnet50) ",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Human-Part-Segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plot_predictions(val_images[:4], colormap, model=model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deeplabv3_plus",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}