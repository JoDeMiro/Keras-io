{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Augmenting convnets with aggregated attention\n",
    "\n",
    "**Author:** [Aritra Roy Gosthipaty](https://twitter.com/ariG23498)<br>\n",
    "**Date created:** 2022/01/22<br>\n",
    "**Last modified:** 2022/01/22<br>\n",
    "**Description:** Building a patch-convnet architecture and visualizing its attention maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Vision transformers ([Dosovitskiy et. al](https://arxiv.org/abs/2010.11929))\n",
    "have emerged as a powerful alternative to Convolutional Neural Networks.\n",
    "ViTs process the images in a patch-based manner. The image information\n",
    "is then aggregated into a `CLASS` token. This token correlates to the\n",
    "most important patches of the image for a particular classification decision.\n",
    "\n",
    "The interaction between the `CLASS` token and the patches can be visualized\n",
    "to help explain a classification decision. In the academic paper\n",
    "[Augmenting convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\n",
    "by Touvron et. al, the authors propose to set up an equivalent visualization for\n",
    "convnets. They propose to substitute the global average pooling layer\n",
    "of a convnet with a Transformer layer. The self-attention layer of the\n",
    "Transformer would produces attention maps that correspond to the\n",
    "most attended patches of the image for the classification decision.\n",
    "\n",
    "In this example, we minimally implement the ideas of\n",
    "[Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692).\n",
    "The main goal of this example is to cover the following ideas, with\n",
    "minor modifications (to adjust the implementation with CIFAR10):\n",
    "\n",
    "- The simple design for the attention-based pooling layer, such that\n",
    "    it explicitly provides the weights (importance) of the different\n",
    "    patches.\n",
    "- The novel architecture of convnet called the **PatchConvNet** which\n",
    "    deviates from the age old pyramidal architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup and Imports\n",
    "\n",
    "This example requires TensorFlow Addons, which can be installed using\n",
    "the following command:\n",
    "\n",
    "```shell\n",
    "pip install -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set seed for reproducibiltiy\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = BATCH_SIZE * 2\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 10  # for CIFAR 10\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 48  # We will resize input images to this size.\n",
    "\n",
    "# ARCHITECTURE\n",
    "DIMENSIONS = 256\n",
    "SE_RATIO = 8\n",
    "TRUNK_DEPTH = 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# PRETRAINING\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_val, y_val) = (\n",
    "    (x_train[:40000], y_train[:40000]),\n",
    "    (x_train[40000:], y_train[40000:]),\n",
    ")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Augmentation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_preprocessing():\n",
    "    model = keras.Sequential(\n",
    "        [layers.Rescaling(1 / 255.0), layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),],\n",
    "        name=\"preprocessing\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_train_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Rescaling(1 / 255.0),\n",
    "            layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "            layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "        ],\n",
    "        name=\"train_data_augmentation\",\n",
    "    )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Convolutional stem\n",
    "\n",
    "The stem of the model is a lightweight preprocessing module that\n",
    "maps images pixels to a set of vectors (patches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_convolutional_stem(dimensions):\n",
    "    \"\"\"Build the convolutional stem.\n",
    "\n",
    "    Args:\n",
    "        dimensions: The embedding dimension of the patches (d in paper).\n",
    "\n",
    "    Returs:\n",
    "        The convolutional stem as a keras seqeuntial\n",
    "        model.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"kernel_size\": (3, 3),\n",
    "        \"strides\": (2, 2),\n",
    "        \"activation\": tf.nn.gelu,\n",
    "        \"padding\": \"same\",\n",
    "    }\n",
    "\n",
    "    convolutional_stem = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(filters=dimensions // 2, **config),\n",
    "            layers.Conv2D(filters=dimensions, **config),\n",
    "        ],\n",
    "        name=\"convolutional_stem\",\n",
    "    )\n",
    "\n",
    "    return convolutional_stem\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Convolutional trunk\n",
    "\n",
    "The trunk of the model is the most compute-intesive part. It consists\n",
    "of `N` stacked residual convolutional blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SqueezeExcite(layers.Layer):\n",
    "    \"\"\"Applies squeeze and excitation to input feature maps as seen in\n",
    "    https://arxiv.org/abs/1709.01507.\n",
    "\n",
    "    Args:\n",
    "        ratio: The ratio with which the feature map needs to be reduced in\n",
    "        the reduction phase.\n",
    "\n",
    "    Inputs:\n",
    "        Convolutional features.\n",
    "\n",
    "    Outputs:\n",
    "        Attention modified feature maps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"ratio\": self.ratio})\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        filters = input_shape[-1]\n",
    "        self.squeeze = layers.GlobalAveragePooling2D(keepdims=True)\n",
    "        self.reduction = layers.Dense(\n",
    "            units=filters // self.ratio, activation=\"relu\", use_bias=False,\n",
    "        )\n",
    "        self.excite = layers.Dense(units=filters, activation=\"sigmoid\", use_bias=False)\n",
    "        self.multiply = layers.Multiply()\n",
    "\n",
    "    def call(self, x):\n",
    "        shortcut = x\n",
    "        x = self.squeeze(x)\n",
    "        x = self.reduction(x)\n",
    "        x = self.excite(x)\n",
    "        x = self.multiply([shortcut, x])\n",
    "        return x\n",
    "\n",
    "\n",
    "class Trunk(layers.Layer):\n",
    "    \"\"\"Convolutional residual trunk as in the https://arxiv.org/abs/2112.13692\n",
    "\n",
    "    Args:\n",
    "        depth: Number of trunk residual blocks\n",
    "        dimensions: Dimnesion of the model (denoted by d in the paper)\n",
    "        ratio: The Squeeze-Excitation ratio\n",
    "\n",
    "    Inputs:\n",
    "        Convolutional features extracted from the conv stem.\n",
    "\n",
    "    Outputs:\n",
    "        Flattened patches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth, dimensions, ratio, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "        self.dimensions = dimensions\n",
    "        self.depth = depth\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\"ratio\": self.ratio, \"dimensions\": self.dimensions, \"depth\": self.depth,}\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        config = {\n",
    "            \"filters\": self.dimensions,\n",
    "            \"activation\": tf.nn.gelu,\n",
    "            \"padding\": \"same\",\n",
    "        }\n",
    "\n",
    "        trunk_block = [\n",
    "            layers.LayerNormalization(epsilon=1e-6),\n",
    "            layers.Conv2D(kernel_size=(1, 1), **config),\n",
    "            layers.Conv2D(kernel_size=(3, 3), **config),\n",
    "            SqueezeExcite(ratio=self.ratio),\n",
    "            layers.Conv2D(kernel_size=(1, 1), filters=self.dimensions, padding=\"same\"),\n",
    "        ]\n",
    "\n",
    "        self.trunk_blocks = [keras.Sequential(trunk_block) for _ in range(self.depth)]\n",
    "        self.add = layers.Add()\n",
    "        self.flatten_spatial = layers.Reshape((-1, self.dimensions))\n",
    "\n",
    "    def call(self, x):\n",
    "        # Remember the input.\n",
    "        shortcut = x\n",
    "        for trunk_block in self.trunk_blocks:\n",
    "            output = trunk_block(x)\n",
    "            shortcut = self.add([output, shortcut])\n",
    "            x = shortcut\n",
    "        # Flatten the patches.\n",
    "        x = self.flatten_spatial(x)\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Attention Pooling\n",
    "\n",
    "The output of the convolutional trunk is attended with a trainable\n",
    "_query_ class token. The resulting attention map is the weight of\n",
    "every patch of the image for a classification decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class AttentionPooling(layers.Layer):\n",
    "    \"\"\"Applies attention to the patches extracted form the\n",
    "    trunk with the CLS token.\n",
    "\n",
    "    Args:\n",
    "        dimensions: The dimension of the whole architecture.\n",
    "        num_classes: The number of classes in the dataset.\n",
    "\n",
    "    Inputs:\n",
    "        Flattened patches from the trunk.\n",
    "\n",
    "    Outputs:\n",
    "        The modifies CLS token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dimensions = dimensions\n",
    "        self.num_classes = num_classes\n",
    "        self.cls = tf.Variable(tf.zeros((1, 1, dimensions)))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"dimensions\": self.dimensions,\n",
    "                \"num_classes\": self.num_classes,\n",
    "                \"cls\": self.cls.numpy(),\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=1, key_dim=self.dimensions, dropout=0.2,\n",
    "        )\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=self.dimensions, activation=tf.nn.gelu),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(units=self.dimensions, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )\n",
    "        self.dense = layers.Dense(units=self.num_classes)\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        # Expand the class token batch number of times.\n",
    "        class_token = tf.repeat(self.cls, repeats=batch_size, axis=0)\n",
    "        # Concat the input with the trainable class token.\n",
    "        x = tf.concat([class_token, x], axis=1)\n",
    "        # Apply attention to x.\n",
    "        x = self.layer_norm1(x)\n",
    "        x, viz_weights = self.attention(\n",
    "            query=x[:, 0:1], key=x, value=x, return_attention_scores=True\n",
    "        )\n",
    "        class_token = class_token + x\n",
    "        class_token = self.layer_norm2(class_token)\n",
    "        class_token = self.flatten(class_token)\n",
    "        class_token = self.layer_norm3(class_token)\n",
    "        class_token = class_token + self.mlp(class_token)\n",
    "        # Build the logits\n",
    "        logits = self.dense(class_token)\n",
    "        return logits, tf.squeeze(viz_weights)[..., 1:]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Patch convnet\n",
    "\n",
    "The patch-convnet is shown in the figure below.\n",
    "\n",
    "| ![image model](https://i.imgur.com/NHiQeac.png) |\n",
    "| :--: |\n",
    "| [Source](https://arxiv.org/abs/2112.13692) |\n",
    "\n",
    "All the modules in the architecture are built in the earlier seciton.\n",
    "In this section, we stack all of the different modules together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchConvNet(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        stem,\n",
    "        trunk,\n",
    "        attention_pooling,\n",
    "        preprocessing_model,\n",
    "        train_augmentation_model,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stem = stem\n",
    "        self.trunk = trunk\n",
    "        self.attention_pooling = attention_pooling\n",
    "        self.train_augmentation_model = train_augmentation_model\n",
    "        self.preprocessing_model = preprocessing_model\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"stem\": self.stem,\n",
    "                \"trunk\": self.trunk,\n",
    "                \"attention_pooling\": self.attention_pooling,\n",
    "                \"train_augmentation_model\": self.train_augmentation_model,\n",
    "                \"preprocessing_model\": self.preprocessing_model,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def _calculate_loss(self, inputs, test=False):\n",
    "        images, labels = inputs\n",
    "        # Augment the input images.\n",
    "        if test:\n",
    "            augmented_images = self.preprocessing_model(images)\n",
    "        else:\n",
    "            augmented_images = self.train_augmentation_model(images)\n",
    "        # Pass through the stem.\n",
    "        x = self.stem(augmented_images)\n",
    "        # Pass through the trunk.\n",
    "        x = self.trunk(x)\n",
    "        # Pass through the attention pooling block.\n",
    "        logits, _ = self.attention_pooling(x)\n",
    "        # Compute the total loss.\n",
    "        total_loss = self.compiled_loss(labels, logits)\n",
    "        return total_loss, logits\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, logits = self._calculate_loss(inputs)\n",
    "        # Apply gradients.\n",
    "        train_vars = [\n",
    "            self.stem.trainable_variables,\n",
    "            self.trunk.trainable_variables,\n",
    "            self.attention_pooling.trainable_variables,\n",
    "        ]\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        trainable_variable_list = []\n",
    "        for (grad, var) in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                trainable_variable_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(trainable_variable_list)\n",
    "        # Report progress.\n",
    "        _, labels = inputs\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        total_loss, logits = self._calculate_loss(inputs, test=True)\n",
    "        # Report progress.\n",
    "        _, labels = inputs\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, images):\n",
    "        # Augment the input images.\n",
    "        augmented_images = self.preprocessing_model(images)\n",
    "        # Pass through the stem.\n",
    "        x = self.stem(augmented_images)\n",
    "        # Pass through the trunk.\n",
    "        x = self.trunk(x)\n",
    "        # Pass through the attention pooling block.\n",
    "        logits, viz_weights = self.attention_pooling(x)\n",
    "        return logits, viz_weights\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Callbacks\n",
    "\n",
    "This callback will plot the image and the attention map overlayed on\n",
    "the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Taking a batch of test inputs to measure model's progress.\n",
    "test_images, test_labels = next(iter(test_ds))\n",
    "\n",
    "\n",
    "class TrainMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_interval=None):\n",
    "        self.epoch_interval = epoch_interval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.epoch_interval and epoch % self.epoch_interval == 4:\n",
    "            test_augmented_images = self.model.preprocessing_model(test_images)\n",
    "            # Pass through the stem.\n",
    "            test_x = self.model.stem(test_augmented_images)\n",
    "            # Pass through the trunk.\n",
    "            test_x = self.model.trunk(test_x)\n",
    "            # Pass through the attention pooling block.\n",
    "            _, test_viz_weights = self.model.attention_pooling(test_x)\n",
    "            # Reshape the vizualization weights\n",
    "            num_patches = tf.shape(test_viz_weights)[-1]\n",
    "            height = width = int(math.sqrt(num_patches))\n",
    "            test_viz_weights = layers.Reshape((height, width))(test_viz_weights)\n",
    "            # Take a random image and its attention weights.\n",
    "            index = np.random.randint(low=0, high=tf.shape(test_augmented_images)[0])\n",
    "            selected_image = test_augmented_images[index]\n",
    "            selected_weight = test_viz_weights[index]\n",
    "            # Plot the images and the overlayed attention map.\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "            ax[0].imshow(selected_image)\n",
    "            ax[0].set_title(f\"Original: {epoch:03d}\")\n",
    "            ax[0].axis(\"off\")\n",
    "            img = ax[1].imshow(selected_image)\n",
    "            ax[1].imshow(\n",
    "                selected_weight, cmap=\"inferno\", alpha=0.6, extent=img.get_extent()\n",
    "            )\n",
    "            ax[1].set_title(f\"Attended: {epoch:03d}\")\n",
    "            ax[1].axis(\"off\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n",
    "\n",
    "\n",
    "total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
    "warmup_epoch_percentage = 0.15\n",
    "warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "scheduled_lrs = WarmUpCosine(\n",
    "    learning_rate_base=LEARNING_RATE,\n",
    "    total_steps=total_steps,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training\n",
    "\n",
    "We build the model, compile it, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_augmentation_model = get_train_augmentation_model()\n",
    "preprocessing_model = get_preprocessing()\n",
    "conv_stem = build_convolutional_stem(dimensions=DIMENSIONS)\n",
    "conv_trunk = Trunk(depth=TRUNK_DEPTH, dimensions=DIMENSIONS, ratio=SE_RATIO)\n",
    "attention_pooling = AttentionPooling(dimensions=DIMENSIONS, num_classes=NUM_CLASSES)\n",
    "\n",
    "patch_conv_net = PatchConvNet(\n",
    "    stem=conv_stem,\n",
    "    trunk=conv_trunk,\n",
    "    attention_pooling=attention_pooling,\n",
    "    train_augmentation_model=train_augmentation_model,\n",
    "    preprocessing_model=preprocessing_model,\n",
    ")\n",
    "\n",
    "# Assemble the callbacks.\n",
    "train_callbacks = [TrainMonitor(epoch_interval=5)]\n",
    "# Get the optimizer.\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=scheduled_lrs, weight_decay=WEIGHT_DECAY)\n",
    "# Compile and pretrain the model.\n",
    "patch_conv_net.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")\n",
    "history = patch_conv_net.fit(\n",
    "    train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=train_callbacks,\n",
    ")\n",
    "\n",
    "# Evaluate the model with the test dataset.\n",
    "loss, acc_top1, acc_top5 = patch_conv_net.evaluate(test_ds)\n",
    "print(f\"Loss: {loss:0.2f}\")\n",
    "print(f\"Top 1 test accuracy: {acc_top1*100:0.2f}%\")\n",
    "print(f\"Top 5 test accuracy: {acc_top5*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Here, we use the trained model to plot the attention map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_attention(image):\n",
    "    \"\"\"Plots the attention map on top of the image.\n",
    "\n",
    "    Args:\n",
    "        image: A numpy image of arbitrary size.\n",
    "    \"\"\"\n",
    "    # Resize the image to a (32, 32) dim.\n",
    "    image = tf.image.resize(image, (32, 32))\n",
    "    image = image[tf.newaxis, ...]\n",
    "    test_augmented_images = patch_conv_net.preprocessing_model(image)\n",
    "    # Pass through the stem.\n",
    "    test_x = patch_conv_net.stem(test_augmented_images)\n",
    "    # Pass through the trunk.\n",
    "    test_x = patch_conv_net.trunk(test_x)\n",
    "    # Pass through the attention pooling block.\n",
    "    _, test_viz_weights = patch_conv_net.attention_pooling(test_x)\n",
    "    test_viz_weights = test_viz_weights[tf.newaxis, ...]\n",
    "    # Reshape the vizualization weights.\n",
    "    num_patches = tf.shape(test_viz_weights)[-1]\n",
    "    height = width = int(math.sqrt(num_patches))\n",
    "    test_viz_weights = layers.Reshape((height, width))(test_viz_weights)\n",
    "    selected_image = test_augmented_images[0]\n",
    "    selected_weight = test_viz_weights[0]\n",
    "    # Plot the images.\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    ax[0].imshow(selected_image)\n",
    "    ax[0].set_title(f\"Original\")\n",
    "    ax[0].axis(\"off\")\n",
    "    img = ax[1].imshow(selected_image)\n",
    "    ax[1].imshow(selected_weight, cmap=\"inferno\", alpha=0.6, extent=img.get_extent())\n",
    "    ax[1].set_title(f\"Attended\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "url = \"http://farm9.staticflickr.com/8017/7140384795_385b1f48df_z.jpg\"\n",
    "image_name = keras.utils.get_file(fname=\"image.jpg\", origin=url)\n",
    "image = tf.io.read_file(image_name)\n",
    "image = tf.io.decode_image(image)\n",
    "plot_attention(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "The attention map corresponding to the trainable `CLASS`\n",
    "token and the patches of the image helps explain the classificaiton\n",
    "decision. One should also note that the attention maps gradually get\n",
    "better. In the initial training regime, the attention is scattered all\n",
    "around while at a later stage, it focuses more on the objects of the\n",
    "image.\n",
    "\n",
    "The non-pyramidal convnet achieves an accuracy of ~84-85% top-1 test\n",
    "accuracy.\n",
    "\n",
    "I would like to thank [JarvisLabs.ai](https://jarvislabs.ai/) for\n",
    "providing GPU credits for this project.\n",
    "\n",
    "You can try the model on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/patch-conv-net)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "patch_conv_net",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}