{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Image Super-Resolution using an Efficient Sub-Pixel CNN\n",
    "\n",
    "**Author:** [Xingyu Long](https://github.com/xingyu-long)<br>\n",
    "**Date created:** 2020/07/28<br>\n",
    "**Last modified:** 2020/08/27<br>\n",
    "**Description:** Implementing Super-Resolution using Efficient sub-pixel model on BSDS500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "ESPCN (Efficient Sub-Pixel CNN), proposed by [Shi, 2016](https://arxiv.org/abs/1609.05158)\n",
    "is a model that reconstructs a high-resolution version of an image given a low-resolution version.\n",
    "It leverages efficient \"sub-pixel convolution\" layers, which learns an array of\n",
    "image upscaling filters.\n",
    "\n",
    "In this code example, we will implement the model from the paper and train it on a small dataset,\n",
    "[BSDS500](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load data: BSDS500 dataset\n",
    "\n",
    "### Download dataset\n",
    "\n",
    "We use the built-in `keras.utils.get_file` utility to retrieve the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "dataset_url = \"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\"\n",
    "data_dir = keras.utils.get_file(origin=dataset_url, fname=\"BSR\", untar=True)\n",
    "root_dir = os.path.join(data_dir, \"BSDS500/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We create training and validation datasets via `image_dataset_from_directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "crop_size = 300\n",
    "upscale_factor = 3\n",
    "input_size = crop_size // upscale_factor\n",
    "batch_size = 8\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    root_dir,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(crop_size, crop_size),\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    label_mode=None,\n",
    ")\n",
    "\n",
    "valid_ds = image_dataset_from_directory(\n",
    "    root_dir,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(crop_size, crop_size),\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    label_mode=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We rescale the images to take values in the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scaling(input_image):\n",
    "    input_image = input_image / 255.0\n",
    "    return input_image\n",
    "\n",
    "\n",
    "# Scale from (0, 255) to (0, 1)\n",
    "train_ds = train_ds.map(scaling)\n",
    "valid_ds = valid_ds.map(scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's visualize a few sample images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    for img in batch:\n",
    "        display(array_to_img(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We prepare a dataset of test image paths that we will use for\n",
    "visual evaluation at the end of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "dataset = os.path.join(root_dir, \"images\")\n",
    "test_path = os.path.join(dataset, \"test\")\n",
    "\n",
    "test_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(test_path, fname)\n",
    "        for fname in os.listdir(test_path)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Crop and resize images\n",
    "\n",
    "Let's process image data.\n",
    "First, we convert our images from the RGB color space to the\n",
    "[YUV colour space](https://en.wikipedia.org/wiki/YUV).\n",
    "\n",
    "For the input data (low-resolution images),\n",
    "we crop the image, retrieve the `y` channel (luninance),\n",
    "and resize it with the `area` method (use `BICUBIC` if you use PIL).\n",
    "We only consider the luminance channel\n",
    "in the YUV color space because humans are more sensitive to\n",
    "luminance change.\n",
    "\n",
    "For the target data (high-resolution images), we just crop the image\n",
    "and retrieve the `y` channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use TF Ops to process.\n",
    "def process_input(input, input_size, upscale_factor):\n",
    "    input = tf.image.rgb_to_yuv(input)\n",
    "    last_dimension_axis = len(input.shape) - 1\n",
    "    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n",
    "    return tf.image.resize(y, [input_size, input_size], method=\"area\")\n",
    "\n",
    "\n",
    "def process_target(input):\n",
    "    input = tf.image.rgb_to_yuv(input)\n",
    "    last_dimension_axis = len(input.shape) - 1\n",
    "    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n",
    "    return y\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "train_ds = train_ds.prefetch(buffer_size=32)\n",
    "\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n",
    ")\n",
    "valid_ds = valid_ds.prefetch(buffer_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's take a look at the input and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    for img in batch[0]:\n",
    "        display(array_to_img(img))\n",
    "    for img in batch[1]:\n",
    "        display(array_to_img(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Build a model\n",
    "\n",
    "Compared to the paper, we add one more layer and we use the `relu` activation function\n",
    "instead of `tanh`.\n",
    "It achieves better performance even though we train the model for fewer epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model(upscale_factor=3, channels=1):\n",
    "    conv_args = {\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_initializer\": \"Orthogonal\",\n",
    "        \"padding\": \"same\",\n",
    "    }\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    x = layers.Conv2D(64, 5, **conv_args)(inputs)\n",
    "    x = layers.Conv2D(64, 3, **conv_args)(x)\n",
    "    x = layers.Conv2D(32, 3, **conv_args)(x)\n",
    "    x = layers.Conv2D(channels * (upscale_factor ** 2), 3, **conv_args)(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscale_factor)\n",
    "\n",
    "    return keras.Model(inputs, outputs)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define utility functions\n",
    "\n",
    "We need to define several utility functions to monitor our results:\n",
    "\n",
    "- `plot_results` to plot an save an image.\n",
    "- `get_lowres_image` to convert an image to its low-resolution version.\n",
    "- `upscale_image` to turn a low-resolution image to\n",
    "a high-resolution version reconstructed by the model.\n",
    "In this function, we use the `y` channel from the YUV color space\n",
    "as input to the model and then combine the output with the\n",
    "other channels to obtain an RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "import PIL\n",
    "\n",
    "\n",
    "def plot_results(img, prefix, title):\n",
    "    \"\"\"Plot the result with zoom-in area.\"\"\"\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Create a new figure with a default 111 subplot.\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(img_array[::-1], origin=\"lower\")\n",
    "\n",
    "    plt.title(title)\n",
    "    # zoom-factor: 2.0, location: upper-left\n",
    "    axins = zoomed_inset_axes(ax, 2, loc=2)\n",
    "    axins.imshow(img_array[::-1], origin=\"lower\")\n",
    "\n",
    "    # Specify the limits.\n",
    "    x1, x2, y1, y2 = 200, 300, 100, 200\n",
    "    # Apply the x-limits.\n",
    "    axins.set_xlim(x1, x2)\n",
    "    # Apply the y-limits.\n",
    "    axins.set_ylim(y1, y2)\n",
    "\n",
    "    plt.yticks(visible=False)\n",
    "    plt.xticks(visible=False)\n",
    "\n",
    "    # Make the line.\n",
    "    mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"blue\")\n",
    "    plt.savefig(str(prefix) + \"-\" + title + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_lowres_image(img, upscale_factor):\n",
    "    \"\"\"Return low-resolution image to use as model input.\"\"\"\n",
    "    return img.resize(\n",
    "        (img.size[0] // upscale_factor, img.size[1] // upscale_factor),\n",
    "        PIL.Image.BICUBIC,\n",
    "    )\n",
    "\n",
    "\n",
    "def upscale_image(model, img):\n",
    "    \"\"\"Predict the result based on input image and restore the image as RGB.\"\"\"\n",
    "    ycbcr = img.convert(\"YCbCr\")\n",
    "    y, cb, cr = ycbcr.split()\n",
    "    y = img_to_array(y)\n",
    "    y = y.astype(\"float32\") / 255.0\n",
    "\n",
    "    input = np.expand_dims(y, axis=0)\n",
    "    out = model.predict(input)\n",
    "\n",
    "    out_img_y = out[0]\n",
    "    out_img_y *= 255.0\n",
    "\n",
    "    # Restore the image in RGB color space.\n",
    "    out_img_y = out_img_y.clip(0, 255)\n",
    "    out_img_y = out_img_y.reshape((np.shape(out_img_y)[0], np.shape(out_img_y)[1]))\n",
    "    out_img_y = PIL.Image.fromarray(np.uint8(out_img_y), mode=\"L\")\n",
    "    out_img_cb = cb.resize(out_img_y.size, PIL.Image.BICUBIC)\n",
    "    out_img_cr = cr.resize(out_img_y.size, PIL.Image.BICUBIC)\n",
    "    out_img = PIL.Image.merge(\"YCbCr\", (out_img_y, out_img_cb, out_img_cr)).convert(\n",
    "        \"RGB\"\n",
    "    )\n",
    "    return out_img\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define callbacks to monitor training\n",
    "\n",
    "The `ESPCNCallback` object will compute and display\n",
    "the [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio) metric.\n",
    "This is the main metric we use to evaluate super-resolution performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ESPCNCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(ESPCNCallback, self).__init__()\n",
    "        self.test_img = get_lowres_image(load_img(test_img_paths[0]), upscale_factor)\n",
    "\n",
    "    # Store PSNR value in each epoch.\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.psnr = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Mean PSNR for epoch: %.2f\" % (np.mean(self.psnr)))\n",
    "        if epoch % 20 == 0:\n",
    "            prediction = upscale_image(self.model, self.test_img)\n",
    "            plot_results(prediction, \"epoch-\" + str(epoch), \"prediction\")\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        self.psnr.append(10 * math.log10(1 / logs[\"loss\"]))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define `ModelCheckpoint` and `EarlyStopping` callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)\n",
    "\n",
    "checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "model = get_model(upscale_factor=upscale_factor, channels=1)\n",
    "model.summary()\n",
    "\n",
    "callbacks = [ESPCNCallback(), early_stopping_callback, model_checkpoint_callback]\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=loss_fn,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds, epochs=epochs, callbacks=callbacks, validation_data=valid_ds, verbose=2\n",
    ")\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Run model prediction and plot the results\n",
    "\n",
    "Let's compute the reconstructed version of a few images and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "total_bicubic_psnr = 0.0\n",
    "total_test_psnr = 0.0\n",
    "\n",
    "for index, test_img_path in enumerate(test_img_paths[50:60]):\n",
    "    img = load_img(test_img_path)\n",
    "    lowres_input = get_lowres_image(img, upscale_factor)\n",
    "    w = lowres_input.size[0] * upscale_factor\n",
    "    h = lowres_input.size[1] * upscale_factor\n",
    "    highres_img = img.resize((w, h))\n",
    "    prediction = upscale_image(model, lowres_input)\n",
    "    lowres_img = lowres_input.resize((w, h))\n",
    "    lowres_img_arr = img_to_array(lowres_img)\n",
    "    highres_img_arr = img_to_array(highres_img)\n",
    "    predict_img_arr = img_to_array(prediction)\n",
    "    bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)\n",
    "    test_psnr = tf.image.psnr(predict_img_arr, highres_img_arr, max_val=255)\n",
    "\n",
    "    total_bicubic_psnr += bicubic_psnr\n",
    "    total_test_psnr += test_psnr\n",
    "\n",
    "    print(\n",
    "        \"PSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr\n",
    "    )\n",
    "    print(\"PSNR of predict and high resolution is %.4f\" % test_psnr)\n",
    "    plot_results(lowres_img, index, \"lowres\")\n",
    "    plot_results(highres_img, index, \"highres\")\n",
    "    plot_results(prediction, index, \"prediction\")\n",
    "\n",
    "print(\"Avg. PSNR of lowres images is %.4f\" % (total_bicubic_psnr / 10))\n",
    "print(\"Avg. PSNR of reconstructions is %.4f\" % (total_test_psnr / 10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "super_resolution_sub_pixel",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}