{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Investigating Vision Transformer representations\n",
    "\n",
    "**Authors:** [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Sayak Paul](https://twitter.com/RisingSayak) (equal contribution)<br>\n",
    "**Date created:** 2022/04/12<br>\n",
    "**Last modified:** 2022/04/17<br>\n",
    "**Description:** Looking into the representations learned by different Vision Transformers variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we look into the representations learned by different Vision\n",
    "Transformer (ViT) models. Our main goal with this example is to provide insights into\n",
    "what empowers ViTs to learn from image data. In particular, the example discusses\n",
    "implementations of a few different ViT analysis tools.\n",
    "\n",
    "**Note:** when we say \"Vision Transformer\", we refer to a computer vision architecture that\n",
    "involves Transformer blocks ([Vaswani et al.](https://arxiv.org/abs/1706.03762)) and not\n",
    "necessarily the original Vision Transformer model\n",
    "([Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)).\n",
    "\n",
    "## Models considered\n",
    "\n",
    "Since the inception of the original Vision Transformer, the computer vision community has\n",
    "seen a number of different ViT variants improving upon the original in various ways:\n",
    "training improvements, architecture improvements, and so on.\n",
    "In this example, we consider the following ViT model families:\n",
    "\n",
    "* ViTs trained using supervised pretraining with the ImageNet-1k and ImageNet-21k\n",
    "datasets ([Dosovitskiy et al.](https://arxiv.org/abs/2010.11929))\n",
    "* ViTs trained using supervised pretraining but only with the ImageNet-1k dataset with\n",
    "more regularization and distillation ([Touvron et al.](https://arxiv.org/abs/2012.12877))\n",
    "(DeiT).\n",
    "* ViTs trained using self-supervised pretraining ([Caron et al.](https://arxiv.org/abs/2104.14294))\n",
    "(DINO).\n",
    "\n",
    "Since the pretrained models are not implemented in Keras, we first implemented them as\n",
    "faithfully as possible. We then populated them with the official pretrained parameters.\n",
    "Finally, we evaluated our implementations on the ImageNet-1k validation set to ensure the\n",
    "evaluation numbers were matching with the original implementations. The details of our implementations\n",
    "are available in [this repository](https://github.com/sayakpaul/probing-vits).\n",
    "\n",
    "To keep the example concise, we won't exhaustively pair each model with the analysis\n",
    "methods. We'll provide notes in the respective sections so that you can pick up the\n",
    "pieces.\n",
    "\n",
    "To run this example on Google Colab, we need to update the `gdown` library like so:\n",
    "\n",
    "```shell\n",
    "pip install -U gdown -q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "RESOLUTION = 224\n",
    "PATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Data utilities\n",
    "\n",
    "For the original ViT models, the input images need to be scaled to the range `[-1, 1]`. For\n",
    "the other model families mentioned at the beginning, we need to normalize the images with\n",
    "channel-wise mean and standard deviation of the ImageNet-1k training set.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "crop_layer = keras.layers.CenterCrop(RESOLUTION, RESOLUTION)\n",
    "norm_layer = keras.layers.Normalization(\n",
    "    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "    variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
    ")\n",
    "rescale_layer = keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1)\n",
    "\n",
    "\n",
    "def preprocess_image(image, model_type, size=RESOLUTION):\n",
    "    # Turn the image into a numpy array and add batch dim.\n",
    "    image = np.array(image)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    # If model type is vit rescale the image to [-1, 1].\n",
    "    if model_type == \"original_vit\":\n",
    "        image = rescale_layer(image)\n",
    "\n",
    "    # Resize the image using bicubic interpolation.\n",
    "    resize_size = int((256 / 224) * size)\n",
    "    image = tf.image.resize(image, (resize_size, resize_size), method=\"bicubic\")\n",
    "\n",
    "    # Crop the image.\n",
    "    image = crop_layer(image)\n",
    "\n",
    "    # If model type is DeiT or DINO normalize the image.\n",
    "    if model_type != \"original_vit\":\n",
    "        image = norm_layer(image)\n",
    "\n",
    "    return image.numpy()\n",
    "\n",
    "\n",
    "def load_image_from_url(url, model_type):\n",
    "    # Credit: Willi Gierke\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    preprocessed_image = preprocess_image(image, model_type)\n",
    "    return image, preprocessed_image\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load a test image and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# ImageNet-1k label mapping file and load it.\n",
    "\n",
    "mapping_file = keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\"\n",
    ")\n",
    "\n",
    "with open(mapping_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "imagenet_int_to_str = [line.rstrip() for line in lines]\n",
    "\n",
    "img_url = \"https://dl.fbaipublicfiles.com/dino/img.png\"\n",
    "image, preprocessed_image = load_image_from_url(img_url, model_type=\"original_vit\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_tfhub_model(model_url: str) -> tf.keras.Model:\n",
    "    inputs = keras.Input((RESOLUTION, RESOLUTION, 3))\n",
    "    hub_module = hub.KerasLayer(model_url)\n",
    "    outputs, attention_weights = hub_module(inputs)\n",
    "    return keras.Model(inputs, outputs=[outputs, attention_weights])\n",
    "\n",
    "\n",
    "def get_gdrive_model(model_id: str) -> tf.keras.Model:\n",
    "    model_path = gdown.download(id=model_id, quiet=False)\n",
    "    with zipfile.ZipFile(model_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    model_name = model_path.split(\".\")[0]\n",
    "    inputs = keras.Input((RESOLUTION, RESOLUTION, 3))\n",
    "    model = keras.models.load_model(model_name, compile=False)\n",
    "    outputs, attention_weights = model(inputs)\n",
    "    return keras.Model(inputs, outputs=[outputs, attention_weights])\n",
    "\n",
    "\n",
    "def get_model(url_or_id):\n",
    "    if \"https\" in url_or_id:\n",
    "        loaded_model = get_tfhub_model(url_or_id)\n",
    "    else:\n",
    "        loaded_model = get_gdrive_model(url_or_id)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "vit_base_i21k_patch16_224 = get_model(\"1mbtnliT3jRb3yJUHhbItWw8unfYZw8KJ\")\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**More about the model**:\n",
    "\n",
    "This model was pretrained on the ImageNet-21k dataset and was then fine-tuned on the\n",
    "ImageNet-1k dataset. To learn more about how we developed this model in TensorFlow\n",
    "(with pretrained weights from\n",
    "[this source](https://github.com/google-research/vision_transformer/)) refer to\n",
    "[this notebook](https://github.com/sayakpaul/probing-vits/blob/main/notebooks/load-jax-weights-vitb16.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Running regular inference with the model\n",
    "\n",
    "We now run inference with the loaded model on our test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "predictions, attention_score_dict = vit_base_i21k_patch16_224.predict(\n",
    "    preprocessed_image\n",
    ")\n",
    "predicted_label = imagenet_int_to_str[int(np.argmax(predictions))]\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`attention_score_dict` contains the attention scores (softmaxed outputs) from each\n",
    "attention head of each Transformer block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Method I: Mean attention distance\n",
    "\n",
    "[Dosovitskiy et al.](https://arxiv.org/abs/2010.11929) and\n",
    "[Raghu et al.](https://arxiv.org/abs/2108.08810) use a measure called\n",
    "\"mean attention distance\" from each attention head of different\n",
    "Transformer blocks to understand how local and global information flows\n",
    "into Vision Transformers.\n",
    "\n",
    "Mean attention distance is defined as the distance between query tokens and the other\n",
    "tokens times attention weights. So, for a single image\n",
    "\n",
    "* we take individual patches (tokens) extracted from the image,\n",
    "* calculate their geometric distance, and\n",
    "* multiply that with the attention scores.\n",
    "\n",
    "Attention scores are calculated here after forward passing the image in inference mode\n",
    "through the network. The following figure may help you  understand the process a\n",
    "little bit better.\n",
    "\n",
    "<img src=\"https://i.imgur.com/pZCgPwl.gif\" height=500>\n",
    "\n",
    "This animation is created by [Ritwik Raha](https://twitter.com/ritwik_raha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_distance_matrix(patch_size, num_patches, length):\n",
    "    distance_matrix = np.zeros((num_patches, num_patches))\n",
    "    for i in range(num_patches):\n",
    "        for j in range(num_patches):\n",
    "            if i == j:  # zero distance\n",
    "                continue\n",
    "\n",
    "            xi, yi = (int(i / length)), (i % length)\n",
    "            xj, yj = (int(j / length)), (j % length)\n",
    "            distance_matrix[i, j] = patch_size * np.linalg.norm([xi - xj, yi - yj])\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "def compute_mean_attention_dist(patch_size, attention_weights, model_type):\n",
    "    num_cls_tokens = 2 if \"distilled\" in model_type else 1\n",
    "\n",
    "    # The attention_weights shape = (batch, num_heads, num_patches, num_patches)\n",
    "    attention_weights = attention_weights[\n",
    "        ..., num_cls_tokens:, num_cls_tokens:\n",
    "    ]  # Removing the CLS token\n",
    "    num_patches = attention_weights.shape[-1]\n",
    "    length = int(np.sqrt(num_patches))\n",
    "    assert length**2 == num_patches, \"Num patches is not perfect square\"\n",
    "\n",
    "    distance_matrix = compute_distance_matrix(patch_size, num_patches, length)\n",
    "    h, w = distance_matrix.shape\n",
    "\n",
    "    distance_matrix = distance_matrix.reshape((1, 1, h, w))\n",
    "    # The attention_weights along the last axis adds to 1\n",
    "    # this is due to the fact that they are softmax of the raw logits\n",
    "    # summation of the (attention_weights * distance_matrix)\n",
    "    # should result in an average distance per token.\n",
    "    mean_distances = attention_weights * distance_matrix\n",
    "    mean_distances = np.sum(\n",
    "        mean_distances, axis=-1\n",
    "    )  # Sum along last axis to get average distance per token\n",
    "    mean_distances = np.mean(\n",
    "        mean_distances, axis=-1\n",
    "    )  # Now average across all the tokens\n",
    "\n",
    "    return mean_distances\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Thanks to [Simon Kornblith](https://scholar.google.com/citations?user=1O3RPmsAAAAJ&hl=en)\n",
    "from Google who helped us with this code snippet. It can be found\n",
    "[here](https://gist.github.com/simonster/155894d48aef2bd36bd2dd8267e62391). Let's now use\n",
    "these utilities to generate a plot of attention distances with our loaded model and test\n",
    "image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Build the mean distances for every Transformer block.\n",
    "mean_distances = {\n",
    "    f\"{name}_mean_dist\": compute_mean_attention_dist(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        attention_weights=attention_weight,\n",
    "        model_type=\"original_vit\",\n",
    "    )\n",
    "    for name, attention_weight in attention_score_dict.items()\n",
    "}\n",
    "\n",
    "# Get the number of heads from the mean distance output.\n",
    "num_heads = tf.shape(mean_distances[\"transformer_block_0_att_mean_dist\"])[-1].numpy()\n",
    "\n",
    "# Print the shapes\n",
    "print(f\"Num Heads: {num_heads}.\")\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "for idx in range(len(mean_distances)):\n",
    "    mean_distance = mean_distances[f\"transformer_block_{idx}_att_mean_dist\"]\n",
    "    x = [idx] * num_heads\n",
    "    y = mean_distance[0, :]\n",
    "    plt.scatter(x=x, y=y, label=f\"transformer_block_{idx}\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Attention Head\", fontsize=14)\n",
    "plt.ylabel(\"Attention Distance\", fontsize=14)\n",
    "plt.title(\"vit_base_i21k_patch16_224\", fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Inspecting the plots\n",
    "\n",
    "**How does self-attention span across the input space? Do they attend\n",
    "input regions locally or globally?**\n",
    "\n",
    "The promise of self-attention is to enable the learning of contextual dependencies\n",
    "so that a model can attend to the regions of inputs which are the most salient w.r.t\n",
    "the objective. From the above plots we can notice that different attention heads yield\n",
    "different attention distances suggesting they use both local and global information\n",
    "from an image. But as we go deeper in the Transformer blocks the heads tend to\n",
    "focus more on global aggregate information.\n",
    "\n",
    "Inspired by [Raghu et al.](https://arxiv.org/abs/2108.08810) we computed mean attention\n",
    "distances over 1000 images randomly taken from the ImageNet-1k validation set and we\n",
    "repeated the process for all the models mentioned at the beginning. Intrestingly, we\n",
    "notice the following:\n",
    "\n",
    "* Pretraining with a larger dataset helps with more global attention spans:\n",
    "\n",
    "\n",
    "| Pretrained on ImageNet-21k<br>Fine-tuned on ImageNet-1k | Pretrained on ImageNet-1k |\n",
    "| :--: | :--: |\n",
    "| ![](https://drive.google.com/uc?export=view&id=1aFob5Cj0FkRyVhH4Iw7Dh5SFxQH3rpYs) | ![](https://drive.google.com/uc?export=view&id=13Y-3Ypi58PPRqd-pqP1oHkyNkRHCYypH) |\n",
    "\n",
    "\n",
    "* When distilled from a CNN ViTs tend to have less global attention spans:\n",
    "\n",
    "\n",
    "| No distillation (ViT B-16 from DeiT) | Distilled ViT B-16 from DeiT |\n",
    "| :--: | :--: |\n",
    "| ![](https://drive.google.com/uc?export=view&id=1yH4cPQcMFCnuo3-IW3S9Baszr_d0o2Se) | ![](https://drive.google.com/uc?export=view&id=1m_nG12kq7E_zIEkxhsi7U0Xr_VDXhJYE) |\n",
    "\n",
    "To reproduce these plots, please refer to\n",
    "[this notebook](https://github.com/sayakpaul/probing-vits/blob/main/notebooks/mean-attention-distance-1k.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Method II: Attention Rollout\n",
    "\n",
    "[Abnar et al.](https://arxiv.org/abs/2005.00928) introduce \"Attention rollout\" for\n",
    "quantifying how information flow through self-attention layers of Transformer blocks.\n",
    "Original ViT authors use this method to investigate the learned representations, stating:\n",
    "\n",
    "> Briefly, we averaged attention weights of ViTL/16 across all heads and then recursively\n",
    "multiplied the weight matrices of all layers. This accounts for the mixing of attention\n",
    "across tokens through all layers.\n",
    "\n",
    "We used\n",
    "[this notebook](https://colab.research.google.com/github/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb)\n",
    "and modified the attention rollout code from it for compatibility with our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def attention_rollout_map(image, attention_score_dict, model_type):\n",
    "    num_cls_tokens = 2 if \"distilled\" in model_type else 1\n",
    "\n",
    "    # Stack the individual attention matrices from individual Transformer blocks.\n",
    "    attn_mat = tf.stack([attention_score_dict[k] for k in attention_score_dict.keys()])\n",
    "    attn_mat = tf.squeeze(attn_mat, axis=1)\n",
    "\n",
    "    # Average the attention weights across all heads.\n",
    "    attn_mat = tf.reduce_mean(attn_mat, axis=1)\n",
    "\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    residual_attn = tf.eye(attn_mat.shape[1])\n",
    "    aug_attn_mat = attn_mat + residual_attn\n",
    "    aug_attn_mat = aug_attn_mat / tf.reduce_sum(aug_attn_mat, axis=-1)[..., None]\n",
    "    aug_attn_mat = aug_attn_mat.numpy()\n",
    "\n",
    "    # Recursively multiply the weight matrices.\n",
    "    joint_attentions = np.zeros(aug_attn_mat.shape)\n",
    "    joint_attentions[0] = aug_attn_mat[0]\n",
    "\n",
    "    for n in range(1, aug_attn_mat.shape[0]):\n",
    "        joint_attentions[n] = np.matmul(aug_attn_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "    # Attention from the output token to the input space.\n",
    "    v = joint_attentions[-1]\n",
    "    grid_size = int(np.sqrt(aug_attn_mat.shape[-1]))\n",
    "    mask = v[0, num_cls_tokens:].reshape(grid_size, grid_size)\n",
    "    mask = cv2.resize(mask / mask.max(), image.size)[..., np.newaxis]\n",
    "    result = (mask * image).astype(\"uint8\")\n",
    "    return result\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's now use these utilities to generate an attention plot based on our previous results\n",
    "from the \"Running regular inference with the model\" section. Following are the links to\n",
    "download each individual model:\n",
    "\n",
    "* [Original ViT model (pretrained on ImageNet-21k)](https://drive.google.com/file/d/1mbtnliT3jRb3yJUHhbItWw8unfYZw8KJ/view?usp=sharing)\n",
    "* [Original ViT model (pretrained on ImageNet-1k)](https://drive.google.com/file/d/1ApOdYe4NXxhPhJABefgZ3KVvqsQzhCL7/view?usp=sharing)\n",
    "* [DINO model (pretrained on ImageNet-1k)](https://drive.google.com/file/d/16_1oDm0PeCGJ_KGBG5UKVN7TsAtiRNrN/view?usp=sharing)\n",
    "* [DeiT models (pretrained on ImageNet-1k including distilled and non-distilled ones)](https://tfhub.dev/sayakpaul/collections/deit/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "attn_rollout_result = attention_rollout_map(\n",
    "    image, attention_score_dict, model_type=\"original_vit\"\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 10))\n",
    "fig.suptitle(f\"Predicted label: {predicted_label}.\", fontsize=20)\n",
    "\n",
    "_ = ax1.imshow(image)\n",
    "_ = ax2.imshow(attn_rollout_result)\n",
    "ax1.set_title(\"Input Image\", fontsize=16)\n",
    "ax2.set_title(\"Attention Map\", fontsize=16)\n",
    "ax1.axis(\"off\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=1.35)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Inspecting the plots\n",
    "\n",
    "**How can we quanitfy the information flow that propagates through the\n",
    "attention layers?**\n",
    "\n",
    "We notice that the model is able to focus its attention on the\n",
    "salient parts of the input image. We encourage you to apply this\n",
    "method to the other models we mentioned and compare the results. The\n",
    "attention rollout plots will differ according to the tasks and\n",
    "augmentation the model was trained with. We observe that DeiT has the\n",
    "best rollout plot, likely due to its augmentation regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Method III: Attention heatmaps\n",
    "\n",
    "A simple yet useful way to probe into the representation of a Vision Transformer is to\n",
    "visualise the attention maps overlayed on the input images. This helps form an intuition\n",
    "about what the model attends to. We use the DINO model for this purpose, because it\n",
    "yields better attention heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "vit_dino_base16 = get_model(\"16_1oDm0PeCGJ_KGBG5UKVN7TsAtiRNrN\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Preprocess the same image but with normlization.\n",
    "img_url = \"https://dl.fbaipublicfiles.com/dino/img.png\"\n",
    "image, preprocessed_image = load_image_from_url(img_url, model_type=\"dino\")\n",
    "\n",
    "# Grab the predictions.\n",
    "predictions, attention_score_dict = vit_dino_base16.predict(preprocessed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "A Transformer block consists of multiple heads. Each head in a Transformer block projects\n",
    "the input data to different sub-spaces. This helps each individual head to attend to\n",
    "different parts of the image. Therefore, it makes sense to visualize each attention head\n",
    "map seperately, to make sense of what each heads looks at.\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "* The following code has been copy-modified from the\n",
    "[original DINO codebase](https://github.com/facebookresearch/dino/blob/main/visualize_attention.py).\n",
    "* Here we grab the attention maps of the last Transformer block.\n",
    "* [DINO](https://arxiv.org/abs/2104.14294) was pretrained using a self-supervised\n",
    "objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def attention_heatmap(attention_score_dict, image, model_type=\"dino\"):\n",
    "    num_tokens = 2 if \"distilled\" in model_type else 1\n",
    "\n",
    "    # Sort the Transformer blocks in order of their depth.\n",
    "    attention_score_list = list(attention_score_dict.keys())\n",
    "    attention_score_list.sort(key=lambda x: int(x.split(\"_\")[-2]), reverse=True)\n",
    "\n",
    "    # Process the attention maps for overlay.\n",
    "    w_featmap = image.shape[2] // PATCH_SIZE\n",
    "    h_featmap = image.shape[1] // PATCH_SIZE\n",
    "    attention_scores = attention_score_dict[attention_score_list[0]]\n",
    "\n",
    "    # Taking the representations from CLS token.\n",
    "    attentions = attention_scores[0, :, 0, num_tokens:].reshape(num_heads, -1)\n",
    "\n",
    "    # Reshape the attention scores to resemble mini patches.\n",
    "    attentions = attentions.reshape(num_heads, w_featmap, h_featmap)\n",
    "    attentions = attentions.transpose((1, 2, 0))\n",
    "\n",
    "    # Resize the attention patches to 224x224 (224: 14x16).\n",
    "    attentions = tf.image.resize(\n",
    "        attentions, size=(h_featmap * PATCH_SIZE, w_featmap * PATCH_SIZE)\n",
    "    )\n",
    "    return attentions\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We can use the same image we used for inference with DINO and the `attention_score_dict`\n",
    "we extracted from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# De-normalize the image for visual clarity.\n",
    "in1k_mean = tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
    "in1k_std = tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "preprocessed_img_orig = (preprocessed_image * in1k_std) + in1k_mean\n",
    "preprocessed_img_orig = preprocessed_img_orig / 255.0\n",
    "preprocessed_img_orig = tf.clip_by_value(preprocessed_img_orig, 0.0, 1.0).numpy()\n",
    "\n",
    "# Generate the attention heatmaps.\n",
    "attentions = attention_heatmap(attention_score_dict, preprocessed_img_orig)\n",
    "\n",
    "# Plot the maps.\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(13, 13))\n",
    "img_count = 0\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if img_count < len(attentions):\n",
    "            axes[i, j].imshow(preprocessed_img_orig[0])\n",
    "            axes[i, j].imshow(attentions[..., img_count], cmap=\"inferno\", alpha=0.6)\n",
    "            axes[i, j].title.set_text(f\"Attention head: {img_count}\")\n",
    "            axes[i, j].axis(\"off\")\n",
    "            img_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Inspecting the plots\n",
    "\n",
    "**How can we qualitatively evaluate the attention weights?**\n",
    "\n",
    "The attention weights of a Transformer block are computed between the\n",
    "key and the query. The weights quantifies how important is the key to the query.\n",
    "In the ViTs the key and the query comes from the same image, hence\n",
    "the weights determine which part of the image is important.\n",
    "\n",
    "Plotting the attention weigths overlayed on the image gives us a great\n",
    "intuition about the parts of the image that are important to the Transformer.\n",
    "This plot qualitatively evaluates the purpose of the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Method IV: Visualizing the learned projection filters\n",
    "\n",
    "After extracting non-overlapping patches, ViTs flatten those patches across their\n",
    "saptial dimensions, and then linearly project them. One might wonder, how do these\n",
    "projections look like? Below, we take the ViT B-16 model and visualize its\n",
    "learned projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Extract the projections.\n",
    "projections = (\n",
    "    vit_base_i21k_patch16_224.layers[1]\n",
    "    .get_layer(\"projection\")\n",
    "    .get_layer(\"conv_projection\")\n",
    "    .kernel.numpy()\n",
    ")\n",
    "projection_dim = projections.shape[-1]\n",
    "patch_h, patch_w, patch_channels = projections.shape[:-1]\n",
    "\n",
    "# Scale the projections.\n",
    "scaled_projections = MinMaxScaler().fit_transform(\n",
    "    projections.reshape(-1, projection_dim)\n",
    ")\n",
    "\n",
    "# Reshape the scaled projections so that the leading\n",
    "# three dimensions resemble an image.\n",
    "scaled_projections = scaled_projections.reshape(patch_h, patch_w, patch_channels, -1)\n",
    "\n",
    "# Visualize the first 128 filters of the learned\n",
    "# projections.\n",
    "fig, axes = plt.subplots(nrows=8, ncols=16, figsize=(13, 8))\n",
    "img_count = 0\n",
    "limit = 128\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(16):\n",
    "        if img_count < limit:\n",
    "            axes[i, j].imshow(scaled_projections[..., img_count])\n",
    "            axes[i, j].axis(\"off\")\n",
    "            img_count += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Inspecting the plots\n",
    "\n",
    "**What do the projection filters learn?**\n",
    "\n",
    "[When visualized](https://distill.pub/2017/feature-visualization/),\n",
    "the kernels of a convolutional neural network show\n",
    "the pattern that they look for in an image. This could be circles,\n",
    "sometimes lines -- when combined together (in later stage of a ConvNet), the filters\n",
    "transform into more complex shapes. We have found a stark similarity between such\n",
    "ConvNet kernels and the projection filters of a ViT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Method V: Visualizing the positional emebddings\n",
    "\n",
    "Transformers are permutation-invariant. This means that do not take into account\n",
    "the spatial position of the input tokens. To overcome this\n",
    "limitation, we add positional information to the input tokens.\n",
    "\n",
    "The positional information can be in the form of leaned positional\n",
    "embeddings or handcrafted constant embeddings. In our case, all the\n",
    "three variants of ViTs feature learned positional embeddings.\n",
    "\n",
    "In this section, we visualize the similarities between the\n",
    "learned positional embeddings with itself. Below, we take the ViT B-16\n",
    "model and visualize the similarity of the positional embeddings by\n",
    "taking their dot-product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "position_embeddings = vit_base_i21k_patch16_224.layers[1].positional_embedding.numpy()\n",
    "\n",
    "# Discard the batch dimension and the position embeddings of the\n",
    "# cls token.\n",
    "position_embeddings = position_embeddings.squeeze()[1:, ...]\n",
    "\n",
    "similarity = position_embeddings @ position_embeddings.T\n",
    "plt.imshow(similarity, cmap=\"inferno\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Inspecting the plots\n",
    "\n",
    "**What do the positional embeddings tell us?**\n",
    "\n",
    "The plot has a distinctive diagonal pattern. The main diagonal is the brightest\n",
    "signifying that a position is the most similar to itself. An interesting\n",
    "pattern to look out for is the repeating diagonals. The repeating pattern\n",
    "portrays a sinusoidal function which is close in essence to what was proposed by\n",
    "[Vaswani et. al.](https://arxiv.org/abs/1706.03762) as a hand-crafted feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Notes\n",
    "\n",
    "* DINO extended the attention heatmap generation process to videos. We also\n",
    "[applied](https://github.com/sayakpaul/probing-vits/blob/main/notebooks/dino-attention-map\n",
    "s-video.ipynb) our DINO implementation on a series of videos and obtained similar\n",
    "results. Here's one such video of attention heatmaps:\n",
    "\n",
    "  ![dino](https://i.imgur.com/kAShjbs.gif)\n",
    "\n",
    "* [Raghu et al.](https://arxiv.org/abs/2108.08810) use an array of techniques to\n",
    "investigate the representations learned by ViTs and make comparisons with that of\n",
    "ResNets. We strongly recommend reading their work.\n",
    "\n",
    "* To author this example, we developed\n",
    "[this repository](https://github.com/sayakpaul/probing-vits) to guide our readers so that they\n",
    "can easily reproduce the experiments and extend them.\n",
    "\n",
    "* Another repository that you may find interesting in this regard is\n",
    "[`vit-explain`](https://github.com/jacobgil/vit-explain).\n",
    "\n",
    "* One can also plot the attention rollout and attention heat maps with\n",
    "custom images using our Hugging Face spaces.\n",
    "\n",
    "| Attention Heat Maps | Attention Rollout |\n",
    "| :--: | :--: |\n",
    "| [![Generic badge](https://img.shields.io/badge/\ud83e\udd17%20Spaces-Attention%20Heat%20Maps-black.svg)](https://huggingface.co/spaces/probing-vits/attention-heat-maps) | [![Generic badge](https://img.shields.io/badge/\ud83e\udd17%20Spaces-Attention%20Rollout-black.svg)](https://huggingface.co/spaces/probing-vits/attention-rollout) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "- [PyImageSearch](https://pyimagesearch.com)\n",
    "- [Jarvislabs.ai](https://jarvislabs.ai/)\n",
    "- [GDE Program](https://developers.google.com/programs/experts/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "probing_vits",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}