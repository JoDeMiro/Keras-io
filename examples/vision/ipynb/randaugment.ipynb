{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# RandAugment for Image Classification for Improved Robustness\n",
    "\n",
    "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
    "**Date created:** 2021/03/13<br>\n",
    "**Last modified:** 2021/03/17<br>\n",
    "**Description:** RandAugment for training an image classification model with improved robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Data augmentation is a very useful technique that can help to improve the translational\n",
    "invariance of convolutional neural networks (CNN). RandAugment is a stochastic data\n",
    "augmentation routine for vision data and was proposed in\n",
    "[RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/abs/1909.13719).\n",
    "It is composed of strong augmentation transforms like color jitters, Gaussian blurs,\n",
    "saturations, etc. along with more traditional augmentation transforms such as\n",
    "random crops.\n",
    "\n",
    "RandAugment has two parameters:\n",
    "\n",
    "* `n` that denotes the number of randomly selected augmentation transforms to apply\n",
    "sequentially\n",
    "* `m` strength of all the augmentation transforms\n",
    "\n",
    "These parameters are tuned for a given dataset and a network architecture. The authors of\n",
    "RandAugment also provide pseudocode of RandAugment in the original paper (Figure 2).\n",
    "\n",
    "Recently, it has been a key component of works like\n",
    "[Noisy Student Training](https://arxiv.org/abs/1911.04252) and\n",
    "[Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848).\n",
    "It has been also central to the\n",
    "success of [EfficientNets](https://arxiv.org/abs/1905.11946).\n",
    "\n",
    "This example requires TensorFlow 2.4 or higher, as well as\n",
    "[`imgaug`](https://imgaug.readthedocs.io/),\n",
    "which can be installed using the following command:\n",
    "\n",
    "```python\n",
    "pip install imgaug\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "tf.random.set_seed(42)\n",
    "ia.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load the CIFAR10 dataset\n",
    "\n",
    "For this example, we will be using the\n",
    "[CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Total training examples: {len(x_train)}\")\n",
    "print(f\"Total test examples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "IMAGE_SIZE = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Initialize `RandAugment` object\n",
    "\n",
    "Now, we will initialize a `RandAugment` object from the `imgaug.augmenters` module with\n",
    "the parameters suggested by the RandAugment authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "rand_aug = iaa.RandAugment(n=3, m=7)\n",
    "\n",
    "\n",
    "def augment(images):\n",
    "    # Input to `augment()` is a TensorFlow tensor which\n",
    "    # is not supported by `imgaug`. This is why we first\n",
    "    # convert it to its `numpy` variant.\n",
    "    images = tf.cast(images, tf.uint8)\n",
    "    return rand_aug(images=images.numpy())\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create TensorFlow `Dataset` objects\n",
    "\n",
    "Because `RandAugment` can only process NumPy arrays, it\n",
    "cannot be applied directly as part of the `Dataset` object (which expects TensorFlow\n",
    "tensors). To make `RandAugment` part of the dataset, we need to wrap it in a\n",
    "[`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function).\n",
    "\n",
    "A `tf.py_function` is a TensorFlow operation (which, like any other TensorFlow operation,\n",
    "takes TF tensors as arguments and returns TensorFlow tensors) that is capable of running\n",
    "arbitrary Python code. Naturally, this Python code can only be executed on CPU (whereas\n",
    "the rest of the TensorFlow graph can be accelerated on GPU), which in some  cases can\n",
    "cause significant slowdowns -- however, in this case, the `Dataset` pipeline will run\n",
    "asynchronously together with the model, and doing preprocessing on CPU will remain\n",
    "performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_ds_rand = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(\n",
    "        lambda x, y: (tf.image.resize(x, (IMAGE_SIZE, IMAGE_SIZE)), y),\n",
    "        num_parallel_calls=AUTO,\n",
    "    )\n",
    "    .map(\n",
    "        lambda x, y: (tf.py_function(augment, [x], [tf.float32])[0], y),\n",
    "        num_parallel_calls=AUTO,\n",
    "    )\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(\n",
    "        lambda x, y: (tf.image.resize(x, (IMAGE_SIZE, IMAGE_SIZE)), y),\n",
    "        num_parallel_calls=AUTO,\n",
    "    )\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Note about using `tf.py_function`**:\n",
    "\n",
    "* As our `augment()` function is not a native TensorFlow operation chances are likely\n",
    "that it can turn into an expensive operation. This is why it is much better to apply it\n",
    "_after_ batching our dataset.\n",
    "* `tf.py_function` is [not compatible](https://github.com/tensorflow/tensorflow/issues/38762)\n",
    "with TPUs. So, if you have distributed TensorFlow training pipelines that use TPUs\n",
    "you cannot use `tf.py_function`. In that case, consider switching to a multi-GPU environment,\n",
    "or rewriting the contents of the function in pure TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For comparison purposes, let's also define a simple augmentation pipeline consisting of\n",
    "random flips, random rotations, and random zoomings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "simple_aug = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now, map the augmentation pipeline to our training dataset\n",
    "train_ds_simple = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y: (simple_aug(x), y), num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualize the dataset augmented with RandAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sample_images, _ = next(iter(train_ds_rand))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image in enumerate(sample_images[:9]):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "You are encouraged to run the above code block a couple of times to see different\n",
    "variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualize the dataset augmented with `simple_aug`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sample_images, _ = next(iter(train_ds_simple))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image in enumerate(sample_images[:9]):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define a model building utility function\n",
    "\n",
    "Now, we define a CNN model that is based on the\n",
    "[ResNet50V2 architecture](https://arxiv.org/abs/1603.05027). Also,\n",
    "notice that the network already has a rescaling layer inside it. This eliminates the need\n",
    "to do any separate preprocessing on our dataset and is specifically very useful for\n",
    "deployment purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_training_model():\n",
    "    resnet50_v2 = tf.keras.applications.ResNet50V2(\n",
    "        weights=None,\n",
    "        include_top=True,\n",
    "        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "        classes=10,\n",
    "    )\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Input((IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "            layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "            resnet50_v2,\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "get_training_model().summary()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will train this network on two different versions of our dataset:\n",
    "\n",
    "* One augmented with RandAugment.\n",
    "* Another one augmented with `simple_aug`.\n",
    "\n",
    "Since RandAugment is known to enhance the robustness of models to common perturbations\n",
    "and corruptions, we will also evaluate our models on the CIFAR-10-C dataset, proposed in\n",
    "[Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://arxiv.org/abs/1903.12261)\n",
    "by Hendrycks et al. The CIFAR-10-C dataset\n",
    "consists of 19 different image corruptions and perturbations (for example speckle noise,\n",
    "fog, Gaussian blur, etc.) that too at varying severity levels. For this example we will\n",
    "be using the following configuration:\n",
    "[`cifar10_corrupted/saturate_5`](https://www.tensorflow.org/datasets/catalog/cifar10_corrupted#cifar10_corruptedsaturate_5).\n",
    "The images from this configuration look like so:\n",
    "\n",
    "![](https://storage.googleapis.com/tfds-data/visualization/fig/cifar10_corrupted-saturate_5-1.0.0.png)\n",
    "\n",
    "In the interest of reproducibility, we serialize the initial random weights of our shallow\n",
    "network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "initial_model = get_training_model()\n",
    "initial_model.save_weights(\"initial_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train model with RandAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "rand_aug_model = get_training_model()\n",
    "rand_aug_model.load_weights(\"initial_weights.h5\")\n",
    "rand_aug_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "rand_aug_model.fit(train_ds_rand, validation_data=test_ds, epochs=EPOCHS)\n",
    "_, test_acc = rand_aug_model.evaluate(test_ds)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train model with `simple_aug`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "simple_aug_model = get_training_model()\n",
    "simple_aug_model.load_weights(\"initial_weights.h5\")\n",
    "simple_aug_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "simple_aug_model.fit(train_ds_simple, validation_data=test_ds, epochs=EPOCHS)\n",
    "_, test_acc = simple_aug_model.evaluate(test_ds)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load the CIFAR-10-C dataset and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load and prepare the CIFAR-10-C dataset\n",
    "# (If it's not already downloaded, it takes ~10 minutes of time to download)\n",
    "cifar_10_c = tfds.load(\"cifar10_corrupted/saturate_5\", split=\"test\", as_supervised=True)\n",
    "cifar_10_c = cifar_10_c.batch(BATCH_SIZE).map(\n",
    "    lambda x, y: (tf.image.resize(x, (IMAGE_SIZE, IMAGE_SIZE)), y),\n",
    "    num_parallel_calls=AUTO,\n",
    ")\n",
    "\n",
    "# Evaluate `rand_aug_model`\n",
    "_, test_acc = rand_aug_model.evaluate(cifar_10_c, verbose=0)\n",
    "print(\n",
    "    \"Accuracy with RandAugment on CIFAR-10-C (saturate_5): {:.2f}%\".format(\n",
    "        test_acc * 100\n",
    "    )\n",
    ")\n",
    "\n",
    "# Evaluate `simple_aug_model`\n",
    "_, test_acc = simple_aug_model.evaluate(cifar_10_c, verbose=0)\n",
    "print(\n",
    "    \"Accuracy with simple_aug on CIFAR-10-C (saturate_5): {:.2f}%\".format(\n",
    "        test_acc * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For the purpose of this example, we trained the models for only a single epoch. On the\n",
    "CIFAR-10-C dataset, the model with RandAugment can perform better with a higher accuracy\n",
    "(for example, 76.64% in one experiment) compared with the model trained with `simple_aug`\n",
    "(e.g., 64.80%). RandAugment can also help stabilize the training. You can explore this\n",
    "[notebook](https://nbviewer.jupyter.org/github/sayakpaul/Keras-Examples-RandAugment/blob/main/RandAugment.ipynb) to check some of the results.\n",
    "\n",
    "In the notebook, you may notice that, at the expense of increased training time with RandAugment,\n",
    "we are able to carve out far better performance on the CIFAR-10-C dataset. You can\n",
    "experiment on the other corruption and perturbation settings that come with the\n",
    "run the same CIFAR-10-C dataset and see if RandAugment helps.\n",
    "\n",
    "You can also experiment with the different values of `n` and `m` in the `RandAugment`\n",
    "object. In the [original paper](https://arxiv.org/abs/1909.13719), the authors show\n",
    "the impact of the individual augmentation transforms for a particular task and a range of\n",
    "ablation studies. You are welcome to check them out.\n",
    "\n",
    "RandAugment has shown great progress in improving the robustness of deep models for\n",
    "computer vision as shown in works like [Noisy Student Training](https://arxiv.org/abs/1911.04252) and\n",
    "[FixMatch](https://arxiv.org/abs/2001.07685). This makes RandAugment quite a useful\n",
    "recipe for training different vision models.\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/randaugment) ",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/randaugment).",
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "randaugment",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}