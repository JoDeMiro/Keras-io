{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Self-supervised contrastive learning with SimSiam\n",
    "\n",
    "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
    "**Date created:** 2021/03/19<br>\n",
    "**Last modified:** 2021/03/20<br>\n",
    "**Description:** Implementation of a self-supervised learning method for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Self-supervised learning (SSL) is an interesting branch of study in the field of\n",
    "representation learning. SSL systems try to formulate a supervised signal from a corpus\n",
    "of unlabeled data points.  An example is we train a deep neural network to predict the\n",
    "next word from a given set of words. In literature, these tasks are known as *pretext\n",
    "tasks* or *auxiliary tasks*. If we [train such a network](https://arxiv.org/abs/1801.06146) on a huge dataset (such as\n",
    "the [Wikipedia text corpus](https://www.corpusdata.org/wikipedia.asp)) it learns very effective\n",
    "representations that transfer well to downstream tasks. Language models like\n",
    "[BERT](https://arxiv.org/abs/1810.04805), [GPT-3](https://arxiv.org/abs/2005.14165),\n",
    "[ELMo](https://allennlp.org/elmo) all benefit from this.\n",
    "\n",
    "Much like the language models we can train computer vision models using similar\n",
    "approaches. To make things work in computer vision, we need to formulate the learning\n",
    "tasks such that the underlying model (a deep neural network) is able to make sense of the\n",
    "semantic information present in vision data. One such task is to a model to _contrast_\n",
    "between two different versions of the same image. The hope is that in this way the model\n",
    "will have learn representations where the similar images are grouped as together possible\n",
    "while the dissimilar images are further away.\n",
    "\n",
    "In this example, we will be implementing one such system called **SimSiam** proposed in\n",
    "[Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566). It\n",
    "is implemented as the following:\n",
    "\n",
    "1. We create two different versions of the same dataset with a stochastic data\n",
    "augmentation pipeline. Note that the random initialization seed needs to be the same\n",
    "during create these versions.\n",
    "2. We take a ResNet without any classification head (**backbone**) and we add a shallow\n",
    "fully-connected network (**projection head**) on top of it. Collectively, this is known\n",
    "as the **encoder**.\n",
    "3. We pass the output of the encoder through a **predictor** which is again a shallow\n",
    "fully-connected network having an\n",
    "[AutoEncoder](https://en.wikipedia.org/wiki/Autoencoder) like structure.\n",
    "4. We then train our encoder to maximize the cosine similarity between the two different\n",
    "versions of our dataset.\n",
    "\n",
    "This example requires TensorFlow 2.4 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 2048\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Total training examples: {len(x_train)}\")\n",
    "print(f\"Total test examples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Defining our data augmentation pipeline\n",
    "\n",
    "As studied in [SimCLR](https://arxiv.org/abs/2002.05709) having the right data\n",
    "augmentation pipeline is critical for SSL systems to work effectively in computer vision.\n",
    "Two particular augmentation transforms that seem to matter the most are: 1.) Random\n",
    "resized crops and 2.) Color distortions. Most of the other SSL systems for computer\n",
    "vision (such as [BYOL](https://arxiv.org/abs/2006.07733),\n",
    "[MoCoV2](https://arxiv.org/abs/2003.04297), [SwAV](https://arxiv.org/abs/2006.09882),\n",
    "etc.) include these in their training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def flip_random_crop(image):\n",
    "    # With random crops we also apply horizontal flipping.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n",
    "    return image\n",
    "\n",
    "\n",
    "def color_jitter(x, strength=[0.4, 0.4, 0.4, 0.1]):\n",
    "    x = tf.image.random_brightness(x, max_delta=0.8 * strength[0])\n",
    "    x = tf.image.random_contrast(\n",
    "        x, lower=1 - 0.8 * strength[1], upper=1 + 0.8 * strength[1]\n",
    "    )\n",
    "    x = tf.image.random_saturation(\n",
    "        x, lower=1 - 0.8 * strength[2], upper=1 + 0.8 * strength[2]\n",
    "    )\n",
    "    x = tf.image.random_hue(x, max_delta=0.2 * strength[3])\n",
    "    # Affine transformations can disturb the natural range of\n",
    "    # RGB images, hence this is needed.\n",
    "    x = tf.clip_by_value(x, 0, 255)\n",
    "    return x\n",
    "\n",
    "\n",
    "def color_drop(x):\n",
    "    x = tf.image.rgb_to_grayscale(x)\n",
    "    x = tf.tile(x, [1, 1, 3])\n",
    "    return x\n",
    "\n",
    "\n",
    "def random_apply(func, x, p):\n",
    "    if tf.random.uniform([], minval=0, maxval=1) < p:\n",
    "        return func(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def custom_augment(image):\n",
    "    # As discussed in the SimCLR paper, the series of augmentation\n",
    "    # transformations (except for random crops) need to be applied\n",
    "    # randomly to impose translational invariance.\n",
    "    image = flip_random_crop(image)\n",
    "    image = random_apply(color_jitter, image, p=0.8)\n",
    "    image = random_apply(color_drop, image, p=0.2)\n",
    "    return image\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "It should be noted that an augmentation pipeline is generally dependent on various\n",
    "properties of the dataset we are dealing with. For example, if images in the dataset are\n",
    "heavily object-centric then taking random crops with a very high probability may hurt the\n",
    "training performance.\n",
    "\n",
    "Let's now apply our augmentation pipeline to our dataset and visualize a few outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Convert the data into TensorFlow `Dataset` objects\n",
    "\n",
    "Here we create two different versions of our dataset *without* any ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ssl_ds_one = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "ssl_ds_one = (\n",
    "    ssl_ds_one.shuffle(1024, seed=SEED)\n",
    "    .map(custom_augment, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "ssl_ds_two = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "ssl_ds_two = (\n",
    "    ssl_ds_two.shuffle(1024, seed=SEED)\n",
    "    .map(custom_augment, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# We then zip both of these datasets.\n",
    "ssl_ds = tf.data.Dataset.zip((ssl_ds_one, ssl_ds_two))\n",
    "\n",
    "# Visualize a few augmented images.\n",
    "sample_images_one = next(iter(ssl_ds_one))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for n in range(25):\n",
    "    ax = plt.subplot(5, 5, n + 1)\n",
    "    plt.imshow(sample_images_one[n].numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Ensure that the different versions of the dataset actually contain\n",
    "# identical images.\n",
    "sample_images_two = next(iter(ssl_ds_two))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for n in range(25):\n",
    "    ax = plt.subplot(5, 5, n + 1)\n",
    "    plt.imshow(sample_images_two[n].numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Notice that the images in `samples_images_one` and `sample_images_two` are essentially\n",
    "the same but are augmented differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Defining the encoder and the predictor\n",
    "\n",
    "We use an implementation of ResNet20 that is specifically configured for the CIFAR10\n",
    "dataset. The code is taken from the\n",
    "[keras-idiomatic-programmer](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/blob/master/zoo/resnet/resnet_cifar10_v2.py) repository. The hyperparameters of\n",
    "these architectures have been referred from Section 3 and Appendix A of [the original\n",
    "paper](https://arxiv.org/abs/2011.10566)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!wget -q https://git.io/JYx2x -O resnet_cifar10_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import resnet_cifar10_v2\n",
    "\n",
    "N = 2\n",
    "DEPTH = N * 9 + 2\n",
    "NUM_BLOCKS = ((DEPTH - 2) // 9) - 1\n",
    "\n",
    "\n",
    "def get_encoder():\n",
    "    # Input and backbone.\n",
    "    inputs = layers.Input((CROP_TO, CROP_TO, 3))\n",
    "    x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(\n",
    "        inputs\n",
    "    )\n",
    "    x = resnet_cifar10_v2.stem(x)\n",
    "    x = resnet_cifar10_v2.learner(x, NUM_BLOCKS)\n",
    "    x = layers.GlobalAveragePooling2D(name=\"backbone_pool\")(x)\n",
    "\n",
    "    # Projection head.\n",
    "    x = layers.Dense(\n",
    "        PROJECT_DIM, use_bias=False, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dense(\n",
    "        PROJECT_DIM, use_bias=False, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
    "    )(x)\n",
    "    outputs = layers.BatchNormalization()(x)\n",
    "    return tf.keras.Model(inputs, outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_predictor():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Note the AutoEncoder-like structure.\n",
    "            layers.Input((PROJECT_DIM,)),\n",
    "            layers.Dense(\n",
    "                LATENT_DIM,\n",
    "                use_bias=False,\n",
    "                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
    "            ),\n",
    "            layers.ReLU(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(PROJECT_DIM),\n",
    "        ],\n",
    "        name=\"predictor\",\n",
    "    )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Defining the (pre-)training loop\n",
    "\n",
    "One of the main reasons behind training networks with these kinds of approaches is to\n",
    "utilize the learned representations for downstream tasks like classification. This is why\n",
    "this particular training phase is also referred to as _pre-training_.\n",
    "\n",
    "We start by defining the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(p, z):\n",
    "    # The authors of SimSiam emphasize the impact of\n",
    "    # the `stop_gradient` operator in the paper as it\n",
    "    # has an important role in the overall optimization.\n",
    "    z = tf.stop_gradient(z)\n",
    "    p = tf.math.l2_normalize(p, axis=1)\n",
    "    z = tf.math.l2_normalize(z, axis=1)\n",
    "    # Negative cosine similarity (minimizing this is\n",
    "    # equivalent to maximizing the similarity).\n",
    "    return -tf.reduce_mean(tf.reduce_sum((p * z), axis=1))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We then define our training loop by overriding the `train_step()` function of the\n",
    "`tf.keras.Model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimSiam(tf.keras.Model):\n",
    "    def __init__(self, encoder, predictor):\n",
    "        super(SimSiam, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.predictor = predictor\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        ds_one, ds_two = data\n",
    "\n",
    "        # Forward pass through the encoder and predictor.\n",
    "        with tf.GradientTape() as tape:\n",
    "            z1, z2 = self.encoder(ds_one), self.encoder(ds_two)\n",
    "            p1, p2 = self.predictor(z1), self.predictor(z2)\n",
    "            # Note that here we are enforcing the network to match\n",
    "            # the representations of two differently augmented batches\n",
    "            # of data.\n",
    "            loss = compute_loss(p1, z2) / 2 + compute_loss(p2, z1) / 2\n",
    "\n",
    "        # Compute gradients and update the parameters.\n",
    "        learnable_params = (\n",
    "            self.encoder.trainable_variables + self.predictor.trainable_variables\n",
    "        )\n",
    "        gradients = tape.gradient(loss, learnable_params)\n",
    "        self.optimizer.apply_gradients(zip(gradients, learnable_params))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Pre-training our networks\n",
    "\n",
    "In the interest of this example, we will train the model for only 5 epochs. In reality,\n",
    "this should at least be 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a cosine decay learning scheduler.\n",
    "num_training_samples = len(x_train)\n",
    "steps = EPOCHS * (num_training_samples // BATCH_SIZE)\n",
    "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.03, decay_steps=steps\n",
    ")\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Compile model and start training.\n",
    "simsiam = SimSiam(get_encoder(), get_predictor())\n",
    "simsiam.compile(optimizer=tf.keras.optimizers.SGD(lr_decayed_fn, momentum=0.6))\n",
    "history = simsiam.fit(ssl_ds, epochs=EPOCHS, callbacks=[early_stopping])\n",
    "\n",
    "# Visualize the training progress of the model.\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.grid()\n",
    "plt.title(\"Negative Cosine Similairty\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "If your solution gets very close to -1 (minimum value of our loss) very quickly with a\n",
    "different dataset and a different backbone architecture that is likely because of\n",
    "*representation collapse*. It is a phenomenon where the encoder yields similar output for\n",
    "all the images. In that case additional hyperparameter tuning is required especially in\n",
    "the following areas:\n",
    "\n",
    "* Strength of the color distortions and their probabilities.\n",
    "* Learning rate and its schedule.\n",
    "* Architecture of both the backbone and their projection head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluating our SSL method\n",
    "\n",
    "The most popularly used method to evaluate a SSL method in computer vision (or any other\n",
    "pre-training method as such) is to learn a linear classifier on the frozen features of\n",
    "the trained backbone model (in this case it is ResNet20) and evaluate the classifier on\n",
    "unseen images. Other methods include\n",
    "[fine-tuning](https://keras.io/guides/transfer_learning/) on the source dataset or even a\n",
    "target dataset with 5% or 10% labels present. Practically, we can use the backbone model\n",
    "for any downstream task such as semantic segmentation, object detection, and so on where\n",
    "the backbone models are usually pre-trained with *pure supervised learning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# We first create labeled `Dataset` objects.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Then we shuffle, batch, and prefetch this dataset for performance. We\n",
    "# also apply random resized crops as an augmentation but only to the\n",
    "# training set.\n",
    "train_ds = (\n",
    "    train_ds.shuffle(1024)\n",
    "    .map(lambda x, y: (flip_random_crop(x), y), num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "# Extract the backbone ResNet20.\n",
    "backbone = tf.keras.Model(\n",
    "    simsiam.encoder.input, simsiam.encoder.get_layer(\"backbone_pool\").output\n",
    ")\n",
    "\n",
    "# We then create our linear classifier and train it.\n",
    "backbone.trainable = False\n",
    "inputs = layers.Input((CROP_TO, CROP_TO, 3))\n",
    "x = backbone(inputs, training=False)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "linear_model = tf.keras.Model(inputs, outputs, name=\"linear_model\")\n",
    "\n",
    "# Compile model and start training.\n",
    "linear_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    optimizer=tf.keras.optimizers.SGD(lr_decayed_fn, momentum=0.9),\n",
    ")\n",
    "history = linear_model.fit(\n",
    "    train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[early_stopping]\n",
    ")\n",
    "_, test_acc = linear_model.evaluate(test_ds)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Notes\n",
    "* More data and longer pre-training schedule benefit SSL in general.\n",
    "* SSL is particularly very helpful when you do not have access to very limited *labeled*\n",
    "training data but you can manage to build a large corpus of unlabeled data. Recently,\n",
    "using an SSL method called [SwAV](https://arxiv.org/abs/2006.09882), a group of\n",
    "researchers at Facebook trained a [RegNet](https://arxiv.org/abs/2006.09882) on 2 Billion\n",
    "images. They were able to achieve downstream performance very close to those achieved by\n",
    "pure supervised pre-training. For some downstream tasks, their method even outperformed\n",
    "the supervised counterparts. You can check out [their\n",
    "paper](https://arxiv.org/pdf/2103.01988.pdf) to know the details.\n",
    "* If you are interested to understand why contrastive SSL helps networks learn meaningful\n",
    "representations, you can check out the following resources:\n",
    "   * [Self-supervised learning: The dark matter of\n",
    "intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)\n",
    "   * [Understanding self-supervised learning using controlled datasets with known\n",
    "structure](https://sslneuips20.github.io/files/CameraReadys%203-77/64/CameraReady/Understanding_self_supervised_learning.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "simsiam",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}