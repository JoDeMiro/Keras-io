{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Keras debugging tips\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/05/16<br>\n",
    "**Last modified:** 2020/05/16<br>\n",
    "**Description:** Four simple tips to help you debug your Keras code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "It's generally possible to do almost anything in Keras *without writing code* per se:\n",
    "whether you're implementing a new type of GAN or the latest convnet architecture for\n",
    "image segmentation, you can usually stick to calling built-in methods. Because all\n",
    "built-in methods do extensive input validation checks, you will have little to no\n",
    "debugging to do. A Functional API model made entirely of built-in layers will work on\n",
    "first try -- if you can compile it, it will run.\n",
    "\n",
    "However, sometimes, you will need to dive deeper and write your own code. Here are some\n",
    "common examples:\n",
    "\n",
    "- Creating a new `Layer` subclass.\n",
    "- Creating a custom `Metric` subclass.\n",
    "- Implementing a custom `train_step` on a `Model`.\n",
    "\n",
    "This document provides a few simple tips to help you navigate debugging in these\n",
    "situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Tip 1: test each part before you test the whole\n",
    "\n",
    "If you've created any object that has a chance of not working as expected, don't just\n",
    "drop it in your end-to-end process and watch sparks fly. Rather, test your custom object\n",
    "in isolation first. This may seem obvious -- but you'd be surprised how often people\n",
    "don't start with this.\n",
    "\n",
    "- If you write a custom layer, don't call `fit()` on your entire model just yet. Call\n",
    "your layer on some test data first.\n",
    "- If you write a custom metric, start by printing its output for some reference inputs.\n",
    "\n",
    "Here's a simple example. Let's write a custom layer a bug in it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class MyAntirectifier(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(output_dim * 2, output_dim),\n",
    "            initializer=\"he_normal\",\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Take the positive part of the input\n",
    "        pos = tf.nn.relu(inputs)\n",
    "        # Take the negative part of the input\n",
    "        neg = tf.nn.relu(-inputs)\n",
    "        # Concatenate the positive and negative parts\n",
    "        concatenated = tf.concat([pos, neg], axis=0)\n",
    "        # Project the concatenation down to the same dimensionality as the input\n",
    "        return tf.matmul(concatenated, self.kernel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, rather than using it in a end-to-end model directly, let's try to call the layer on\n",
    "some test data:\n",
    "\n",
    "```python\n",
    "x = tf.random.normal(shape=(2, 5))\n",
    "y = MyAntirectifier()(x)\n",
    "```\n",
    "\n",
    "We get the following error:\n",
    "\n",
    "```\n",
    "...\n",
    "      1 x = tf.random.normal(shape=(2, 5))\n",
    "----> 2 y = MyAntirectifier()(x)\n",
    "...\n",
    "     17         neg = tf.nn.relu(-inputs)\n",
    "     18         concatenated = tf.concat([pos, neg], axis=0)\n",
    "---> 19         return tf.matmul(concatenated, self.kernel)\n",
    "...\n",
    "InvalidArgumentError: Matrix size-incompatible: In[0]: [4,5], In[1]: [10,5] [Op:MatMul]\n",
    "```\n",
    "\n",
    "Looks like our input tensor in the `matmul` op may have an incorrect shape.\n",
    "Let's add a print statement to check the actual shapes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyAntirectifier(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(output_dim * 2, output_dim),\n",
    "            initializer=\"he_normal\",\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pos = tf.nn.relu(inputs)\n",
    "        neg = tf.nn.relu(-inputs)\n",
    "        print(\"pos.shape:\", pos.shape)\n",
    "        print(\"neg.shape:\", neg.shape)\n",
    "        concatenated = tf.concat([pos, neg], axis=0)\n",
    "        print(\"concatenated.shape:\", concatenated.shape)\n",
    "        print(\"kernel.shape:\", self.kernel.shape)\n",
    "        return tf.matmul(concatenated, self.kernel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We get the following:\n",
    "\n",
    "```\n",
    "pos.shape: (2, 5)\n",
    "neg.shape: (2, 5)\n",
    "concatenated.shape: (4, 5)\n",
    "kernel.shape: (10, 5)\n",
    "```\n",
    "\n",
    "Turns out we had the wrong axis for the `concat` op! We should be concatenating `neg` and\n",
    "`pos` alongside the feature axis 1, not the batch axis 0. Here's the correct version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyAntirectifier(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(output_dim * 2, output_dim),\n",
    "            initializer=\"he_normal\",\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pos = tf.nn.relu(inputs)\n",
    "        neg = tf.nn.relu(-inputs)\n",
    "        print(\"pos.shape:\", pos.shape)\n",
    "        print(\"neg.shape:\", neg.shape)\n",
    "        concatenated = tf.concat([pos, neg], axis=1)\n",
    "        print(\"concatenated.shape:\", concatenated.shape)\n",
    "        print(\"kernel.shape:\", self.kernel.shape)\n",
    "        return tf.matmul(concatenated, self.kernel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now our code works fine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape=(2, 5))\n",
    "y = MyAntirectifier()(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Tip 2: use `model.summary()` and `plot_model()` to check layer output shapes\n",
    "\n",
    "If you're working with complex network topologies, you're going to need a way\n",
    "to visualize how your layers are connected and how they transform the data that passes\n",
    "through them.\n",
    "\n",
    "Here's an example. Consider this model with three inputs and two outputs (lifted from the\n",
    "[Functional API\n",
    "guide](https://keras.io/guides/functional_api/#manipulate-complex-graph-topologies)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "num_tags = 12  # Number of unique issue tags\n",
    "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "num_departments = 4  # Number of departments for predictions\n",
    "\n",
    "title_input = keras.Input(\n",
    "    shape=(None,), name=\"title\"\n",
    ")  # Variable-length sequence of ints\n",
    "body_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
    "tags_input = keras.Input(\n",
    "    shape=(num_tags,), name=\"tags\"\n",
    ")  # Binary vectors of size `num_tags`\n",
    "\n",
    "# Embed each word in the title into a 64-dimensional vector\n",
    "title_features = layers.Embedding(num_words, 64)(title_input)\n",
    "# Embed each word in the text into a 64-dimensional vector\n",
    "body_features = layers.Embedding(num_words, 64)(body_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "title_features = layers.LSTM(128)(title_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "body_features = layers.LSTM(32)(body_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = layers.concatenate([title_features, body_features, tags_input])\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "priority_pred = layers.Dense(1, name=\"priority\")(x)\n",
    "# Stick a department classifier on top of the features\n",
    "department_pred = layers.Dense(num_departments, name=\"department\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = keras.Model(\n",
    "    inputs=[title_input, body_input, tags_input],\n",
    "    outputs=[priority_pred, department_pred],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Calling `summary()` can help you check the output shape of each layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "You can also visualize the entire network topology alongside output shapes using\n",
    "`plot_model`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "With this plot, any connectivity-level error becomes immediately obvious.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Tip 3: to debug what happens during `fit()`, use `run_eagerly=True`\n",
    "\n",
    "The `fit()` method is fast: it runs a well-optimized, fully-compiled computation graph.\n",
    "That's great for performance, but it also means that the code you're executing isn't the\n",
    "Python code you've written. This can be problematic when debugging. As you may recall,\n",
    "Python is slow -- so we use it as a staging language, not as an execution language.\n",
    "\n",
    "Thankfully, there's an easy way to run your code in \"debug mode\", fully eagerly:\n",
    "pass `run_eagerly=True` to `compile()`. Your call to `fit()` will now get executed line\n",
    "by line, without any optimization. It's slower, but it makes it possible to print the\n",
    "value of intermediate tensors, or to use a Python debugger. Great for debugging.\n",
    "\n",
    "Here's a basic example: let's write a really simple model with a custom `train_step`. Our\n",
    "model just implements gradient descent, but instead of first-order gradients, it uses a\n",
    "combination of first-order and second-order gradients. Pretty trivial so far.\n",
    "\n",
    "Can you spot what we're doing wrong?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        trainable_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                preds = self(inputs, training=True)  # Forward pass\n",
    "                # Compute the loss value\n",
    "                # (the loss function is configured in `compile()`)\n",
    "                loss = self.compiled_loss(targets, preds)\n",
    "            # Compute first-order gradients\n",
    "            dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "        # Compute second-order gradients\n",
    "        d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "        # Combine first-order and second-order gradients\n",
    "        grads = [0.5 * w1 + 0.5 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets, preds)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's train a one-layer model on MNIST with this custom training loop.\n",
    "\n",
    "We pick, somewhat at random, a batch size of 1024 and a learning rate of 0.1. The general\n",
    "idea being to use larger batches and a larger learning rate than usual, since our\n",
    "\"improved\" gradients should lead us to quicker convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct an instance of MyModel\n",
    "def get_model():\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    intermediate = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(intermediate)\n",
    "    model = MyModel(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784)) / 255\n",
    "\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=1024, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Oh no, it doesn't converge! Something is not working as planned.\n",
    "\n",
    "Time for some step-by-step printing of what's going on with our gradients.\n",
    "\n",
    "We add various `print` statements in the `train_step` method, and we make sure to pass\n",
    "`run_eagerly=True` to `compile()` to run our code step-by-step, eagerly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        print()\n",
    "        print(\"----Start of step: %d\" % (self.step_counter,))\n",
    "        self.step_counter += 1\n",
    "\n",
    "        inputs, targets = data\n",
    "        trainable_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                preds = self(inputs, training=True)  # Forward pass\n",
    "                # Compute the loss value\n",
    "                # (the loss function is configured in `compile()`)\n",
    "                loss = self.compiled_loss(targets, preds)\n",
    "            # Compute first-order gradients\n",
    "            dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "        # Compute second-order gradients\n",
    "        d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "        print(\"Max of dl_dw[0]: %.4f\" % tf.reduce_max(dl_dw[0]))\n",
    "        print(\"Min of dl_dw[0]: %.4f\" % tf.reduce_min(dl_dw[0]))\n",
    "        print(\"Mean of dl_dw[0]: %.4f\" % tf.reduce_mean(dl_dw[0]))\n",
    "        print(\"-\")\n",
    "        print(\"Max of d2l_dw2[0]: %.4f\" % tf.reduce_max(d2l_dw2[0]))\n",
    "        print(\"Min of d2l_dw2[0]: %.4f\" % tf.reduce_min(d2l_dw2[0]))\n",
    "        print(\"Mean of d2l_dw2[0]: %.4f\" % tf.reduce_mean(d2l_dw2[0]))\n",
    "\n",
    "        # Combine first-order and second-order gradients\n",
    "        grads = [0.5 * w1 + 0.5 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets, preds)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=True,\n",
    ")\n",
    "model.step_counter = 0\n",
    "# We pass epochs=1 and steps_per_epoch=10 to only run 10 steps of training.\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=1024, verbose=0, steps_per_epoch=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "What did we learn?\n",
    "\n",
    "- The first order and second order gradients can have values that differ by orders of\n",
    "magnitudes.\n",
    "- Sometimes, they may not even have the same sign.\n",
    "- Their values can vary greatly at each step.\n",
    "\n",
    "This leads us to an obvious idea: let's normalize the gradients before combining them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        trainable_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                preds = self(inputs, training=True)  # Forward pass\n",
    "                # Compute the loss value\n",
    "                # (the loss function is configured in `compile()`)\n",
    "                loss = self.compiled_loss(targets, preds)\n",
    "            # Compute first-order gradients\n",
    "            dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "        # Compute second-order gradients\n",
    "        d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "        dl_dw = [tf.math.l2_normalize(w) for w in dl_dw]\n",
    "        d2l_dw2 = [tf.math.l2_normalize(w) for w in d2l_dw2]\n",
    "\n",
    "        # Combine first-order and second-order gradients\n",
    "        grads = [0.5 * w1 + 0.5 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets, preds)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=1024, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, training converges! It doesn't work well at all, but at least the model learns\n",
    "something.\n",
    "\n",
    "After spending a few minutes tuning parameters, we get to the following configuration\n",
    "that works somewhat well (achieves 97% validation accuracy and seems reasonably robust to\n",
    "overfitting):\n",
    "\n",
    "- Use `0.2 * w1 + 0.8 * w2` for combining gradients.\n",
    "- Use a learning rate that decays linearly over time.\n",
    "\n",
    "I'm not going to say that the idea works -- this isn't at all how you're supposed to do\n",
    "second-order optimization (pointers: see the Newton & Gauss-Newton methods, quasi-Newton\n",
    "methods, and BFGS). But hopefully this demonstration gave you an idea of how you can\n",
    "debug your way out of uncomfortable training situations.\n",
    "\n",
    "Remember: use `run_eagerly=True` for debugging what happens in `fit()`. And when your code\n",
    "is finally working as expected, make sure to remove this flag in order to get the best\n",
    "runtime performance!\n",
    "\n",
    "Here's our final training run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        trainable_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                preds = self(inputs, training=True)  # Forward pass\n",
    "                # Compute the loss value\n",
    "                # (the loss function is configured in `compile()`)\n",
    "                loss = self.compiled_loss(targets, preds)\n",
    "            # Compute first-order gradients\n",
    "            dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "        # Compute second-order gradients\n",
    "        d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "        dl_dw = [tf.math.l2_normalize(w) for w in dl_dw]\n",
    "        d2l_dw2 = [tf.math.l2_normalize(w) for w in d2l_dw2]\n",
    "\n",
    "        # Combine first-order and second-order gradients\n",
    "        grads = [0.2 * w1 + 0.8 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets, preds)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "lr = learning_rate = keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate=0.1, decay_steps=25, decay_rate=0.1\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(lr),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=2048, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Tip 4: if your code is slow, run the TensorFlow profiler\n",
    "\n",
    "One last tip -- if your code seems slower than it should be, you're going to want to plot\n",
    "how much time is spent on each computation step. Look for any bottleneck that might be\n",
    "causing less than 100% device utilization.\n",
    "\n",
    "To learn more about TensorFlow profiling, see\n",
    "[this extensive guide](https://www.tensorflow.org/guide/profiler).\n",
    "\n",
    "You can quickly profile a Keras model via the TensorBoard callback:\n",
    "\n",
    "```python\n",
    "# Profile from batches 10 to 15\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "                                             profile_batch=(10, 15))\n",
    "# Train the model and use the TensorBoard Keras callback to collect\n",
    "# performance profiling data\n",
    "model.fit(dataset,\n",
    "          epochs=1,\n",
    "          callbacks=[tb_callback])\n",
    "```\n",
    "\n",
    "Then navigate to the TensorBoard app and check the \"profile\" tab.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "debugging_tips",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
