{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Estimating required sample size for model training\n",
    "\n",
    "**Author:** [JacoVerster](https://twitter.com/JacoVerster)<br>\n",
    "**Date created:** 2021/05/20<br>\n",
    "**Last modified:** 2021/06/06<br>\n",
    "**Description:** Modeling the relationship between training set size and model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In many real-world scenarios, the amount image data available to train a deep learning model is\n",
    "limited. This is especially true in the medical imaging domain, where dataset creation is\n",
    "costly. One of the first questions that usually comes up when approaching a new problem is:\n",
    "**\"how many images will we need to train a good enough machine learning model?\"**\n",
    "\n",
    "In most cases, a small set of samples is available, and we can use it to model the relationship\n",
    "between training data size and model performance. Such a model can be used to estimate the optimal\n",
    "number of images needed to arrive at a sample size that would achieve the required model performance.\n",
    "\n",
    "A systematic review of\n",
    "[Sample-Size Determination Methodologies](https://www.researchgate.net/publication/335779941_Sample-Size_Determination_Methodologies_for_Machine_Learning_in_Medical_Imaging_Research_A_Systematic_Review)\n",
    "by Balki et al. provides examples of several sample-size determination methods. In this\n",
    "example, a balanced subsampling scheme is used to determine the optimal sample size for\n",
    "our model. This is done by selecting a random subsample consisting of Y number of images\n",
    "and training the model using the subsample. The model is then evaluated on an independent\n",
    "test set. This process is repeated N times for each subsample with replacement to allow\n",
    "for the construction of a mean and confidence interval for the observed performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define seed and fixed variables\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load TensorFlow dataset and convert to NumPy arrays\n",
    "\n",
    "We'll be using the [TF Flowers dataset](https://www.tensorflow.org/datasets/catalog/tf_flowers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Specify dataset parameters\n",
    "dataset_name = \"tf_flowers\"\n",
    "batch_size = 64\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Load data from tfds and split 10% off for a test set\n",
    "(train_data, test_data), ds_info = tfds.load(\n",
    "    dataset_name,\n",
    "    split=[\"train[:90%]\", \"train[90%:]\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "# Extract number of classes and list of class names\n",
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "class_names = ds_info.features[\"label\"].names\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Convert datasets to NumPy arrays\n",
    "def dataset_to_array(dataset, image_size, num_classes):\n",
    "    images, labels = [], []\n",
    "    for img, lab in dataset.as_numpy_iterator():\n",
    "        images.append(tf.image.resize(img, image_size).numpy())\n",
    "        labels.append(tf.one_hot(lab, num_classes))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "img_train, label_train = dataset_to_array(train_data, image_size, num_classes)\n",
    "img_test, label_test = dataset_to_array(test_data, image_size, num_classes)\n",
    "\n",
    "num_train_samples = len(img_train)\n",
    "print(f\"Number of training samples: {num_train_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Plot a few examples from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "for n in range(30):\n",
    "    ax = plt.subplot(5, 6, n + 1)\n",
    "    plt.imshow(img_test[n].astype(\"uint8\"))\n",
    "    plt.title(np.array(class_names)[label_test[n] == True][0])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Augmentation\n",
    "\n",
    "Define image augmentation using keras preprocessing layers and apply them to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation model\n",
    "image_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(mode=\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.1),\n",
    "        layers.RandomZoom(height_factor=(-0.1, -0)),\n",
    "        layers.RandomContrast(factor=0.1),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply the augmentations to the training images and plot a few examples\n",
    "img_train = image_augmentation(img_train).numpy()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "for n in range(30):\n",
    "    ax = plt.subplot(5, 6, n + 1)\n",
    "    plt.imshow(img_train[n].astype(\"uint8\"))\n",
    "    plt.title(np.array(class_names)[label_train[n] == True][0])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define model building & training functions\n",
    "\n",
    "We create a few convenience functions to build a transfer-learning model, compile and\n",
    "train it and unfreeze layers for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(num_classes, img_size=image_size[0], top_dropout=0.3):\n",
    "    \"\"\"Creates a classifier based on pre-trained MobileNetV2.\n",
    "\n",
    "    Arguments:\n",
    "        num_classes: Int, number of classese to use in the softmax layer.\n",
    "        img_size: Int, square size of input images (defaults is 224).\n",
    "        top_dropout: Int, value for dropout layer (defaults is 0.3).\n",
    "\n",
    "    Returns:\n",
    "        Uncompiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create input and pre-processing layers for MobileNetV2\n",
    "    inputs = layers.Input(shape=(img_size, img_size, 3))\n",
    "    x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(inputs)\n",
    "    model = keras.applications.MobileNetV2(\n",
    "        include_top=False, weights=\"imagenet\", input_tensor=x\n",
    "    )\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.Dropout(top_dropout)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    print(\"Trainable weights:\", len(model.trainable_weights))\n",
    "    print(\"Non_trainable weights:\", len(model.non_trainable_weights))\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_and_train(\n",
    "    model,\n",
    "    training_data,\n",
    "    training_labels,\n",
    "    metrics=[keras.metrics.AUC(name=\"auc\"), \"acc\"],\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    patience=5,\n",
    "    epochs=5,\n",
    "):\n",
    "    \"\"\"Compiles and trains the model.\n",
    "\n",
    "    Arguments:\n",
    "        model: Uncompiled Keras model.\n",
    "        training_data: NumPy Array, trainig data.\n",
    "        training_labels: NumPy Array, trainig labels.\n",
    "        metrics: Keras/TF metrics, requires at least 'auc' metric (default is\n",
    "                `[keras.metrics.AUC(name='auc'), 'acc']`).\n",
    "        optimizer: Keras/TF optimizer (defaults is `keras.optimizers.Adam()).\n",
    "        patience: Int, epochsfor EarlyStopping patience (defaults is 5).\n",
    "        epochs: Int, number of epochs to train (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        Training history for trained Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    stopper = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_auc\",\n",
    "        mode=\"max\",\n",
    "        min_delta=0,\n",
    "        patience=patience,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    history = model.fit(\n",
    "        x=training_data,\n",
    "        y=training_labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[stopper],\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def unfreeze(model, block_name, verbose=0):\n",
    "    \"\"\"Unfreezes Keras model layers.\n",
    "\n",
    "    Arguments:\n",
    "        model: Keras model.\n",
    "        block_name: Str, layer name for example block_name = 'block4'.\n",
    "                    Checks if supplied string is in the layer name.\n",
    "        verbose: Int, 0 means silent, 1 prints out layers trainability status.\n",
    "\n",
    "    Returns:\n",
    "        Keras model with all layers after (and including) the specified\n",
    "        block_name to trainable, excluding BatchNormalization layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unfreeze from block_name onwards\n",
    "    set_trainable = False\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if block_name in layer.name:\n",
    "            set_trainable = True\n",
    "        if set_trainable and not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "            if verbose == 1:\n",
    "                print(layer.name, \"trainable\")\n",
    "        else:\n",
    "            if verbose == 1:\n",
    "                print(layer.name, \"NOT trainable\")\n",
    "    print(\"Trainable weights:\", len(model.trainable_weights))\n",
    "    print(\"Non-trainable weights:\", len(model.non_trainable_weights))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define iterative training function\n",
    "\n",
    "To train a model over several subsample sets we need to create an iterative training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(training_data, training_labels):\n",
    "    \"\"\"Trains the model as follows:\n",
    "\n",
    "    - Trains only the top layers for 10 epochs.\n",
    "    - Unfreezes deeper layers.\n",
    "    - Train for 20 more epochs.\n",
    "\n",
    "    Arguments:\n",
    "        training_data: NumPy Array, trainig data.\n",
    "        training_labels: NumPy Array, trainig labels.\n",
    "\n",
    "    Returns:\n",
    "        Model accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    model = build_model(num_classes)\n",
    "\n",
    "    # Compile and train top layers\n",
    "    history = compile_and_train(\n",
    "        model,\n",
    "        training_data,\n",
    "        training_labels,\n",
    "        metrics=[keras.metrics.AUC(name=\"auc\"), \"acc\"],\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        patience=3,\n",
    "        epochs=10,\n",
    "    )\n",
    "\n",
    "    # Unfreeze model from block 10 onwards\n",
    "    model = unfreeze(model, \"block_10\")\n",
    "\n",
    "    # Compile and train for 20 epochs with a lower learning rate\n",
    "    fine_tune_epochs = 20\n",
    "    total_epochs = history.epoch[-1] + fine_tune_epochs\n",
    "\n",
    "    history_fine = compile_and_train(\n",
    "        model,\n",
    "        training_data,\n",
    "        training_labels,\n",
    "        metrics=[keras.metrics.AUC(name=\"auc\"), \"acc\"],\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        patience=5,\n",
    "        epochs=total_epochs,\n",
    "    )\n",
    "\n",
    "    # Calculate model accuracy on the test set\n",
    "    _, _, acc = model.evaluate(img_test, label_test)\n",
    "    return np.round(acc, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train models iteratively\n",
    "\n",
    "Now that we have model building functions and supporting iterative functions we can train\n",
    "the model over several subsample splits.\n",
    "\n",
    "- We select the subsample splits as 5%, 10%, 25% and 50% of the downloaded dataset. We\n",
    "pretend that only 50% of the actual data is available at present.\n",
    "- We train the model 5 times from scratch at each split and record the accuracy values.\n",
    "\n",
    "Note that this trains 20 models and will take some time. Make sure you have a GPU runtime\n",
    "active.\n",
    "\n",
    "To keep this example lightweight, sample data from a previous training run is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_iteratively(sample_splits=[0.05, 0.1, 0.25, 0.5], iter_per_split=5):\n",
    "    \"\"\"Trains a model iteratively over several sample splits.\n",
    "\n",
    "    Arguments:\n",
    "        sample_splits: List/NumPy array, contains fractions of the trainins set\n",
    "                        to train over.\n",
    "        iter_per_split: Int, number of times to train a model per sample split.\n",
    "\n",
    "    Returns:\n",
    "        Training accuracy for all splits and iterations and the number of samples\n",
    "        used for training at each split.\n",
    "    \"\"\"\n",
    "    # Train all the sample models and calculate accuracy\n",
    "    train_acc = []\n",
    "    sample_sizes = []\n",
    "\n",
    "    for fraction in sample_splits:\n",
    "        print(f\"Fraction split: {fraction}\")\n",
    "        # Repeat training 3 times for each sample size\n",
    "        sample_accuracy = []\n",
    "        num_samples = int(num_train_samples * fraction)\n",
    "        for i in range(iter_per_split):\n",
    "            print(f\"Run {i+1} out of {iter_per_split}:\")\n",
    "            # Create fractional subsets\n",
    "            rand_idx = np.random.randint(num_train_samples, size=num_samples)\n",
    "            train_img_subset = img_train[rand_idx, :]\n",
    "            train_label_subset = label_train[rand_idx, :]\n",
    "            # Train model and calculate accuracy\n",
    "            accuracy = train_model(train_img_subset, train_label_subset)\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            sample_accuracy.append(accuracy)\n",
    "        train_acc.append(sample_accuracy)\n",
    "        sample_sizes.append(num_samples)\n",
    "    return train_acc, sample_sizes\n",
    "\n",
    "\n",
    "# Running the above function produces the following outputs\n",
    "train_acc = [\n",
    "    [0.8202, 0.7466, 0.8011, 0.8447, 0.8229],\n",
    "    [0.861, 0.8774, 0.8501, 0.8937, 0.891],\n",
    "    [0.891, 0.9237, 0.8856, 0.9101, 0.891],\n",
    "    [0.8937, 0.9373, 0.9128, 0.8719, 0.9128],\n",
    "]\n",
    "\n",
    "sample_sizes = [165, 330, 825, 1651]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Learning curve\n",
    "\n",
    "We now plot the learning curve by fitting an exponential curve through the mean accuracy\n",
    "points. We use TF to fit an exponential function through the data.\n",
    "\n",
    "We then extrapolate the learning curve to the predict the accuracy of a model trained on\n",
    "the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def fit_and_predict(train_acc, sample_sizes, pred_sample_size):\n",
    "    \"\"\"Fits a learning curve to model training accuracy results.\n",
    "\n",
    "    Arguments:\n",
    "        train_acc: List/Numpy Array, training accuracy for all model\n",
    "                    training splits and iterations.\n",
    "        sample_sizes: List/Numpy array, number of samples used for training at\n",
    "                    each split.\n",
    "        pred_sample_size: Int, sample size to predict model accuracy based on\n",
    "                        fitted learning curve.\n",
    "    \"\"\"\n",
    "    x = sample_sizes\n",
    "    mean_acc = [np.mean(i) for i in train_acc]\n",
    "    error = [np.std(i) for i in train_acc]\n",
    "\n",
    "    # Define mean squared error cost and exponential curve fit functions\n",
    "    mse = keras.losses.MeanSquaredError()\n",
    "\n",
    "    def exp_func(x, a, b):\n",
    "        return a * x ** b\n",
    "\n",
    "    # Define variables, learning rate and number of epochs for fitting with TF\n",
    "    a = tf.Variable(0.0)\n",
    "    b = tf.Variable(0.0)\n",
    "    learning_rate = 0.01\n",
    "    training_epochs = 5000\n",
    "\n",
    "    # Fit the exponential function to the data\n",
    "    for epoch in range(training_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = exp_func(x, a, b)\n",
    "            cost_function = mse(y_pred, mean_acc)\n",
    "        # Get gradients and compute adjusted weights\n",
    "        gradients = tape.gradient(cost_function, [a, b])\n",
    "        a.assign_sub(gradients[0] * learning_rate)\n",
    "        b.assign_sub(gradients[1] * learning_rate)\n",
    "    print(f\"Curve fit weights: a = {a.numpy()} and b = {b.numpy()}.\")\n",
    "\n",
    "    # We can now estimate the accuracy for pred_sample_size\n",
    "    max_acc = exp_func(pred_sample_size, a, b).numpy()\n",
    "\n",
    "    # Print predicted x value and append to plot values\n",
    "    print(f\"A model accuracy of {max_acc} is predicted for {pred_sample_size} samples.\")\n",
    "    x_cont = np.linspace(x[0], pred_sample_size, 100)\n",
    "\n",
    "    # Build the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.errorbar(x, mean_acc, yerr=error, fmt=\"o\", label=\"Mean acc & std dev.\")\n",
    "    ax.plot(x_cont, exp_func(x_cont, a, b), \"r-\", label=\"Fitted exponential curve.\")\n",
    "    ax.set_ylabel(\"Model clasification accuracy.\", fontsize=12)\n",
    "    ax.set_xlabel(\"Training sample size.\", fontsize=12)\n",
    "    ax.set_xticks(np.append(x, pred_sample_size))\n",
    "    ax.set_yticks(np.append(mean_acc, max_acc))\n",
    "    ax.set_xticklabels(list(np.append(x, pred_sample_size)), rotation=90, fontsize=10)\n",
    "    ax.yaxis.set_tick_params(labelsize=10)\n",
    "    ax.set_title(\"Learning curve: model accuracy vs sample size.\", fontsize=14)\n",
    "    ax.legend(loc=(0.75, 0.75), fontsize=10)\n",
    "    ax.xaxis.grid(True)\n",
    "    ax.yaxis.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # The mean absolute error (MAE) is calculated for curve fit to see how well\n",
    "    # it fits the data. The lower the error the better the fit.\n",
    "    mae = keras.losses.MeanAbsoluteError()\n",
    "    print(f\"The mae for the curve fit is {mae(mean_acc, exp_func(x, a, b)).numpy()}.\")\n",
    "\n",
    "\n",
    "# We use the whole training set to predict the model accuracy\n",
    "fit_and_predict(train_acc, sample_sizes, pred_sample_size=num_train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "From the extrapolated curve we can see that 3303 images will yield an estimated\n",
    "accuracy of about 95%.\n",
    "\n",
    "Now, let's use all the data (3303 images) and train the model to see if our prediction\n",
    "was accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Now train the model with full dataset to get the actual accuracy\n",
    "accuracy = train_model(img_train, label_train)\n",
    "print(f\"A model accuracy of {accuracy} is reached on {num_train_samples} images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We see that a model accuracy of about 94-96%* is reached using 3303 images. This is quite\n",
    "close to our estimate!\n",
    "\n",
    "Even though we used only 50% of the dataset (1651 images) we were able to model the training\n",
    "behaviour of our model and predict the model accuracy for a given amount of images. This same\n",
    "methodology can be used to predict the amount of images needed to reach a desired accuracy.\n",
    "This is very useful when a smaller set of data is available, and it has been shown that\n",
    "convergence on a deep learning model is possible, but more images are needed. The image count\n",
    "prediction can be used to plan and budget for further image collection initiatives."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sample_size_estimate",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}