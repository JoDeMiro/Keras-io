{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Distributed hyperparameter tuning\n",
    "\n",
    "**Authors:** Tom O'Malley, Haifeng Jin<br>\n",
    "**Date created:** 2019/10/24<br>\n",
    "**Last modified:** 2021/06/02<br>\n",
    "**Description:** Tuning the hyperparameters of the models with multiple GPUs and multiple machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install keras-tuner -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "KerasTuner makes it easy to perform distributed hyperparameter search. No\n",
    "changes to your code are needed to scale up from running single-threaded\n",
    "locally to running on dozens or hundreds of workers in parallel. Distributed\n",
    "KerasTuner uses a chief-worker model. The chief runs a service to which the\n",
    "workers report results and query for the hyperparameters to try next. The chief\n",
    "should be run on a single-threaded CPU instance (or alternatively as a separate\n",
    "process on one of the workers).\n",
    "\n",
    "### Configuring distributed mode\n",
    "\n",
    "Configuring distributed mode for KerasTuner only requires setting three\n",
    "environment variables:\n",
    "\n",
    "**KERASTUNER_TUNER_ID**: This should be set to \"chief\" for the chief process.\n",
    "Other workers should be passed a unique ID (by convention, \"tuner0\", \"tuner1\",\n",
    "etc).\n",
    "\n",
    "**KERASTUNER_ORACLE_IP**: The IP address or hostname that the chief service\n",
    "should run on. All workers should be able to resolve and access this address.\n",
    "\n",
    "**KERASTUNER_ORACLE_PORT**: The port that the chief service should run on. This\n",
    "can be freely chosen, but must be a port that is accessible to the other\n",
    "workers. Instances communicate via the [gRPC](https://www.grpc.io) protocol.\n",
    "\n",
    "The same code can be run on all workers. Additional considerations for\n",
    "distributed mode are:\n",
    "\n",
    "- All workers should have access to a centralized file system to which they can\n",
    "write their results.\n",
    "- All workers should be able to access the necessary training and validation\n",
    "data needed for tuning.\n",
    "- To support fault-tolerance, `overwrite` should be kept as `False` in\n",
    "`Tuner.__init__` (`False` is the default).\n",
    "\n",
    "Example bash script for chief service (sample code for `run_tuning.py` at\n",
    "bottom of page):\n",
    "\n",
    "```\n",
    "export KERASTUNER_TUNER_ID=\"chief\"\n",
    "export KERASTUNER_ORACLE_IP=\"127.0.0.1\"\n",
    "export KERASTUNER_ORACLE_PORT=\"8000\"\n",
    "python run_tuning.py\n",
    "```\n",
    "\n",
    "Example bash script for worker:\n",
    "\n",
    "```\n",
    "export KERASTUNER_TUNER_ID=\"tuner0\"\n",
    "export KERASTUNER_ORACLE_IP=\"127.0.0.1\"\n",
    "export KERASTUNER_ORACLE_PORT=\"8000\"\n",
    "python run_tuning.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Data parallelism with `tf.distribute`\n",
    "\n",
    "KerasTuner also supports data parallelism via\n",
    "[tf.distribute](https://www.tensorflow.org/tutorials/distribute/keras). Data\n",
    "parallelism and distributed tuning can be combined. For example, if you have 10\n",
    "workers with 4 GPUs on each worker, you can run 10 parallel trials with each\n",
    "trial training on 4 GPUs by using\n",
    "[tf.distribute.MirroredStrategy](\n",
    "https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy).\n",
    "You can also run each trial on TPUs via\n",
    "[tf.distribute.TPUStrategy](\n",
    "https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy).\n",
    "Currently\n",
    "[tf.distribute.MultiWorkerMirroredStrategy](\n",
    "https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy)\n",
    "is not supported, but support for this is on the roadmap.\n",
    "\n",
    "\n",
    "### Example code\n",
    "\n",
    "When the enviroment variables described above are set, the example below will\n",
    "run distributed tuning and use data parallelism within each trial via\n",
    "`tf.distribute`. The example loads MNIST from `tensorflow_datasets` and uses\n",
    "[Hyperband](https://arxiv.org/abs/1603.06560) for the hyperparameter\n",
    "search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "import keras_tuner\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"Builds a convolutional model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    x = inputs\n",
    "    for i in range(hp.Int(\"conv_layers\", 1, 3, default=3)):\n",
    "        x = tf.keras.layers.Conv2D(\n",
    "            filters=hp.Int(\"filters_\" + str(i), 4, 32, step=4, default=8),\n",
    "            kernel_size=hp.Int(\"kernel_size_\" + str(i), 3, 5),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )(x)\n",
    "\n",
    "        if hp.Choice(\"pooling\" + str(i), [\"max\", \"avg\"]) == \"max\":\n",
    "            x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.AveragePooling2D()(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    if hp.Choice(\"global_pooling\", [\"max\", \"avg\"]) == \"max\":\n",
    "        x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    optimizer = hp.Choice(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    model.compile(\n",
    "        optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = keras_tuner.Hyperband(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=2,\n",
    "    factor=3,\n",
    "    hyperband_iterations=1,\n",
    "    distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    directory=\"results_dir\",\n",
    "    project_name=\"mnist\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape the images to have the channel dimension.\n",
    "x_train = (x_train.reshape(x_train.shape + (1,)) / 255.0)[:1000]\n",
    "y_train = y_train.astype(np.int64)[:1000]\n",
    "x_test = (x_test.reshape(x_test.shape + (1,)) / 255.0)[:100]\n",
    "y_test = y_test.astype(np.int64)[:100]\n",
    "\n",
    "tuner.search(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    steps_per_epoch=600,\n",
    "    validation_data=(x_test, y_test),\n",
    "    validation_steps=100,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\"val_accuracy\")],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "distributed_tuning",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}