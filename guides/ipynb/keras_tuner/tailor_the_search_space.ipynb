{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Tailor the search space\n",
    "\n",
    "**Authors:** Luca Invernizzi, James Long, Francois Chollet, Tom O'Malley, Haifeng Jin<br>\n",
    "**Date created:** 2019/05/31<br>\n",
    "**Last modified:** 2021/10/27<br>\n",
    "**Description:** Tune a subset of the hyperparameters without changing the hypermodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install keras-tuner -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In this guide, we will show how to tailor the search space without changing the\n",
    "`HyperModel` code directly.  For example, you can only tune some of the\n",
    "hyperparameters and keep the rest fixed, or you can override the compile\n",
    "arguments, like `optimizer`, `loss`, and `metrics`.\n",
    "\n",
    "## The default value of a hyperparameter\n",
    "\n",
    "Before we tailor the search space, it is important to know that every\n",
    "hyperparameter has a default value.  This default value is used as the\n",
    "hyperparameter value when not tuning it during our tailoring the search space.\n",
    "\n",
    "Whenever you register a hyperparameter, you can use the `default` argument to\n",
    "specify a default value:\n",
    "\n",
    "```python\n",
    "hp.Int(\"units\", min_value=32, max_value=128, step=32, default=64)\n",
    "```\n",
    "\n",
    "If you don't, hyperparameters always have a default default (for `Int`, it is\n",
    "equal to `min_value`).\n",
    "\n",
    "In the following model-building function, we specified the default value for\n",
    "the `units` hyperparameter as 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            units=hp.Int(\"units\", min_value=32, max_value=128, step=32, default=64)\n",
    "        )\n",
    "    )\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We will reuse this search space in the rest of the tutorial by overriding the\n",
    "hyperparameters without defining a new search space.\n",
    "\n",
    "## Search a few and fix the rest\n",
    "\n",
    "If you have an existing hypermodel, and you want to search over only a few\n",
    "hyperparameters, and keep the rest fixed, you don't have to change the code in\n",
    "the model-building function or the `HyperModel`.  You can pass a\n",
    "`HyperParameters` to the `hyperparameters` argument to the tuner constructor\n",
    "with all the hyperparameters you want to tune.  Specify\n",
    "`tune_new_entries=False` to prevent it from tuning other hyperparameters, the\n",
    "default value of which would be used.\n",
    "\n",
    "In the following example, we only tune the `learning_rate` hyperparameter, and\n",
    "changed its type and value ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    hyperparameters=hp,\n",
    "    # Prevents unlisted parameters from being tuned\n",
    "    tune_new_entries=False,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"search_a_few\",\n",
    ")\n",
    "\n",
    "# Generate random data\n",
    "x_train = np.random.rand(100, 28, 28, 1)\n",
    "y_train = np.random.randint(0, 10, (100, 1))\n",
    "x_val = np.random.rand(20, 28, 28, 1)\n",
    "y_val = np.random.randint(0, 10, (20, 1))\n",
    "\n",
    "# Run the search\n",
    "tuner.search(x_train, y_train, epochs=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "If you summarize the search space, you will see only one hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Fix a few and tune the rest\n",
    "\n",
    "In the example above we showed how to tune only a few hyperparameters and keep\n",
    "the rest fixed.  You can also do the reverse: only fix a few hyperparameters\n",
    "and tune all the rest.\n",
    "\n",
    "In the following example, we fixed the value of the `learning_rate`\n",
    "hyperparameter.  Pass a `hyperparameters` argument with a `Fixed` entry (or any\n",
    "number of `Fixed` entries).  Also remember to specify `tune_new_entries=True`,\n",
    "which allows us to tune the rest of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "hp.Fixed(\"learning_rate\", value=1e-4)\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=True,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"fix_a_few\",\n",
    ")\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "If you summarize the search space, you will see the `learning_rate` is marked\n",
    "as fixed, and the rest of the hyperparameters are being tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Overriding compilation arguments\n",
    "\n",
    "If you have a hypermodel for which you want to change the existing optimizer,\n",
    "loss, or metrics, you can do so by passing these arguments to the tuner\n",
    "constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"sparse_categorical_crossentropy\",],\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"override_compile\",\n",
    ")\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "If you get the best model, you can see the loss function has changed to MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "tuner.get_best_models()[0].loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Tailor the search space of pre-build HyperModels\n",
    "\n",
    "You can also use these techniques with the pre-build models in KerasTuner, like\n",
    "`HyperResNet` or `HyperXception`.  However, to see what hyperparameters are in\n",
    "these pre-build `HyperModel`s, you will have to read the source code.\n",
    "\n",
    "In the following example, we only tune the `learning_rate` of `HyperXception`\n",
    "and fixed all the rest of the hyperparameters. Because the default loss of\n",
    "`HyperXception` is `categorical_crossentropy`, which expect the labels to be\n",
    "one-hot encoded, which doesn't match our raw integer label data, we need to\n",
    "change it by overriding the `loss` in the compile args to\n",
    "`sparse_categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "hypermodel = keras_tuner.applications.HyperXception(input_shape=(28, 28, 1), classes=10)\n",
    "\n",
    "hp = keras_tuner.HyperParameters()\n",
    "\n",
    "# This will override the `learning_rate` parameter with your\n",
    "# own selection of choices\n",
    "hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel,\n",
    "    hyperparameters=hp,\n",
    "    # Prevents unlisted parameters from being tuned\n",
    "    tune_new_entries=False,\n",
    "    # Override the loss.\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "tuner.search(x_train, y_train, epochs=1, validation_data=(x_val, y_val))\n",
    "tuner.search_space_summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tailor_the_search_space",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}